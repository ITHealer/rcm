# Offline Training Configuration

# Data settings
data:
  dir: "dataset"
  lookback_days: 90
  train_test_split:
    test_days: 7
    val_days: 7

time_decay:
  enabled: true
  half_life_days: 7.0        # Half-life for exponential decay
  min_weight: 0.01            # Minimum weight threshold
  
  # Action multipliers (used in DataLoader)
  action_weights:
    view: 0.5                 # Weak signal
    like: 1.0                 # Standard positive
    comment: 1.5              # Strong engagement
    share: 2.0                # Very strong signal
    save: 1.2                 # Bookmark
    hide: -3.0                # Strong negative
    report: -5.0              # Very strong negative
    
# Model settings
model:
  type: "lightgbm"
  params:
    objective: "binary"
    metric: "auc"
    boosting_type: "gbdt"
    num_leaves: 31
    learning_rate: 0.05
    feature_fraction: 0.8
    bagging_fraction: 0.8
    bagging_freq: 5
    min_data_in_leaf: 20
    num_boost_round: 1000
    early_stopping_rounds: 50

# Embedding settings
embeddings:
  model_name: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  embedding_dim: 384
  batch_size: 32
  posts_per_batch: 1000  # Process 1000 posts at a time

# Collaborative Filtering
collaborative_filtering:
  min_interactions: 3
  similarity_metric: "cosine"
  top_k_similar: 100

# FAISS settings
faiss:
  index_type: "IndexFlatIP"  # Inner Product for cosine similarity
  nprobe: 10

# Output settings
output:
  models_dir: "models"
  keep_last_n_versions: 5

# Training schedule
schedule:
  frequency: "weekly"  # weekly, daily
  day_of_week: 0  # 0=Monday, 6=Sunday
  hour: 2  # 2 AM