# # ======================================================================
# # OFFLINE TRAINING CONFIGURATION  (FULL)
# # ======================================================================

# system:
#   name: "recommendation-offline-training"
#   version: "1.1.0"
#   environment: "development"
#   log_level: "INFO"

# data:
#   source: "database"          # database | csv
#   dir: "dataset"

#   # CSV (nếu dùng)
#   csv_files:
#     users: "User.csv"
#     posts: "Post.csv"
#     reactions: "PostReaction.csv"
#     friendships: "Friendship.csv"

#   # Database DSN (Postgres / MySQL)
#   database:
#     url: "postgresql+psycopg2://postgres:123456@localhost:5551/rcm_db"

#   # Khi chạy window (không áp dụng cho full)
#   lookback_days: 14
#   chunk_size: 100000

#   train_test_split:
#     test_days: 3       # gợi ý: khi data rất mỏng, tạm đặt 1
#     val_days: 3        # gợi ý: khi data rất mỏng, tạm đặt 1

# # Mapping bảng trong DB (nếu cần)
# database:
#   url: ""
#   tables:
#     post_view: PostView
#     post_reaction: PostReaction
#     reaction_type: ReactionType
#     user: User
#     post: Post
#     post_hashtag: PostHashtag
#     friendship: Friendship
#     comment: Comment
#   reaction_code_map:
#     like: like
#     love: love
#     laugh: laugh
#     wow: wow
#     sad: sad
#     angry: angry
#     care: care
#   reaction_name_map:
#     Like: like
#     Love: love
#     Laugh: laugh
#     Wow: wow
#     Sad: sad
#     Angry: angry
#     Care: care

# training:
#   # === LỰA CHỌN CHẾ ĐỘ ===
#   mode: "window"             # full | window | incremental
#   pretrain_full_export: false

#   # === WINDOW/INCREMENTAL ===
#   window_days: 14            # enforce TRAIN_WINDOW_DAYS=14 khi mode=window
#   overlap_days: 2            # incremental/window: trừ lùi n ngày chống “đứt quãng”
#   min_train_rows: 1000       # nếu < mức này => cảnh báo/expand window (dev)
#   min_pos_rows: 100

#   # === TIME-DECAY ===
#   half_life_days: 7
#   test_days: 3
#   val_days: 3
#   chunk_size: 100000

#   # multipliers
#   use_action_multiplier: true
#   action_multipliers:
#     view: 0.5
#     like: 1.0
#     love: 1.3
#     care: 1.25
#     laugh: 1.2
#     wow: 1.1
#     sad: 0.9
#     angry: 0.9
#     comment: 1.5
#     share: 2.0
#     save: 1.2

# # EMBEDDINGS / CF / RANKING giữ nguyên như bản của bạn
# embeddings:
#   model_name: "sentence-transformers/all-MiniLM-L6-v2"
#   embedding_dim: 384
#   batch_size: 512
#   normalize: true
#   device: "auto"
#   post:
#     update_frequency: "6h"
#     min_content_length: 10
#   user:
#     lookback_days: 14
#     min_interactions: 5
#     update_frequency: "daily"

# collaborative_filtering:
#   min_interactions: 5
#   top_k_similar_users: 50
#   top_k_similar_items: 50
#   similarity_metric: "cosine"

# ranking_model:
#   algorithm: "lightgbm"
#   params:
#     boosting_type: "gbdt"
#     objective: "binary"
#     metric: "auc"
#     num_leaves: 31
#     max_depth: 6
#     learning_rate: 0.05
#     feature_fraction: 0.8
#     bagging_fraction: 0.8
#     bagging_freq: 5
#     lambda_l1: 0.1
#     lambda_l2: 0.1
#     min_data_in_leaf: 20
#     verbose: -1
#     n_jobs: -1
#     seed: 42
#   num_boost_round: 1000
#   early_stopping_rounds: 50
#   negative_sample_ratio: 5

# features:
#   user_features: 15
#   post_features: 18
#   author_features: 7
#   interaction_features: 7
#   total_features: 47

# artifacts:
#   base_dir: "models"
#   keep_versions: 5
#   # Watermark & meta để incremental/window
#   state_file: "models/training_state.json"
#   save:
#     - embeddings
#     - cf_model
#     - ranking_model
#     - ranking_scaler
#     - ranking_feature_cols
#     - user_stats
#     - author_stats
#     - following_dict
#     - faiss_index
#     - metadata

# schedule:
#   frequency: "weekly"
#   day_of_week: "sunday"
#   time: "02:00"
#   incremental:
#     enabled: true
#     full_retrain_frequency: "biweekly"

# logging:
#   level: "INFO"
#   format: "[%(asctime)s] %(levelname)s - %(name)s - %(message)s"
#   file: "logs/offline_training.log"
#   max_bytes: 10485760
#   backup_count: 5

# monitoring:
#   enabled: true
#   metrics: [training_time, model_auc, feature_importance, data_quality]
#   alerts:
#     min_auc: 0.70
#     max_training_time_hours: 3
#     min_data_size: 10000

# checkpoints:
#   enabled: true
#   dir: "checkpoints"
#   frequency: "hourly"
#   keep_last: 3

# optimization:
#   low_memory_mode: false
#   max_memory_gb: 16
#   n_jobs: -1
#   use_gpu: true
#   gpu_device: 0

# validation:
#   enabled: true
#   metrics: [auc, precision@10, recall@10, ndcg@10, map@10]
#   primary_metric: "auc"
#   minimize: false

# export:
#   online_artifacts: [embeddings, cf_similarities, ranking_model, feature_metadata]
#   format: "pickle"
#   compression: true



# Database connection
database:
  url: "mysql+pymysql://way_root:YmhNWpppahN92AtJotFDoHnCoW38keDp@14.225.220.56:15479/wayjet_system"
  
  # Table names (customize theo DB của bạn)
  tables:
    user: "User"
    post: "Post"
    post_view: "PostView"
    post_reaction: "PostReaction"
    reaction_type: "ReactionType"
    comment: "Comment"
    friendship: "Friendship"

# Training modes
training:
  mode: "full_train"              # Default mode
  window_days: 14                 # For window mode
  overlap_days: 2                 # For incremental (avoid gaps)
  embeddings_lookback_days: 1     # For embeddings_only
  
  # Time decay
  half_life_days: 7.0             # Exponential decay (7 days = 50%)
  min_weight: 0.1                 # Minimum weight floor
  
  # Train/val/test split
  val_days: 3                     # Last 3 days = validation
  test_days: 3                    # Last 3 days of val = test

  # Negative sampling
  negative_ratio: 2.0           # Negative samples per positive (1:2 ratio)
  random_seed: 42

# Embeddings
embeddings:
  model_name: "all-MiniLM-L6-v2"  # Sentence-BERT model
  device: "cpu"                   # 'cpu' or 'cuda'
  batch_size: 32
  min_interactions_for_user: 3 # Min interactions to create user embedding

  # Post content processing
  max_length: 512                  # Max tokens for text
  normalize: true                  # L2 normalize embeddings

# Collaborative Filtering
collaborative_filtering:
  min_interactions: 3             # Min interactions per user
  top_k_similar: 50               # Top-K neighbors
  min_similarity: 0.0             # Min similarity threshold

# Ranking Model (LightGBM)
model_training:
  # LightGBM parameters - OPTIMIZED for small dataset
  learning_rate: 0.01           # ⬇️ Reduced from 0.05 (slower learning)
  num_leaves: 15                # ⬇️ Reduced from 31 (less complex trees)
  max_depth: 4                  # ✅ Added (limit tree depth)
  min_child_samples: 50         # ⬆️ Increased from 20 (more data per leaf)
  n_estimators: 1000            # Keep same
  early_stopping_rounds: 50     # Keep same
  
  # Sampling parameters
  subsample: 0.7                # ⬇️ Reduced from 0.8 (more regularization)
  subsample_freq: 1             # Keep same
  colsample_bytree: 0.7         # ⬇️ Reduced from 0.8 (feature sampling)
  
  # Regularization - STRONG to prevent overfitting
  reg_alpha: 1.0                # ⬆️ Increased from 0.1 (L1 regularization)
  reg_lambda: 1.0               # ⬆️ Increased from 0.1 (L2 regularization)
  min_gain_to_split: 0.1        # ✅ Added (minimum gain for split)
  
  # Training settings
  verbose: -1
  random_seed: 42
  num_threads: -1

# Artifacts
artifacts:
  base_dir: "models"
  keep_versions: 5                # Keep last 5 versions
  state_file: "models/training_state.json"  # Watermark tracking