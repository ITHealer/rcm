# """
# ONLINE INFERENCE PIPELINE - PRODUCTION READY
# =============================================
# Complete recommendation pipeline integrating all components

# Architecture:
# 1. Load artifacts (models, embeddings, stats)
# 2. Initialize recall channels (Following, CF, Content, Trending)
# 3. Initialize ranker & reranker
# 4. Real-time user embedding updates
# 5. Feed generation with < 200ms latency

# Flow: Recall ‚Üí Feature Extraction ‚Üí Ranking ‚Üí Re-ranking ‚Üí Output
# """

# import os
# import sys
# import time
# import numpy as np
# import pandas as pd
# from typing import List, Dict, Optional
# from datetime import datetime
# from pathlib import Path
# import logging
# import yaml
# import warnings
# from collections import defaultdict
# from dataclasses import dataclass

# warnings.filterwarnings('ignore')

# # Add project root
# sys.path.append(str(Path(__file__).parent.parent.parent))

# # Core imports
# from recommender.offline.artifact_manager import ArtifactManager
# from recommender.common.feature_engineer import FeatureEngineer

# # Recall channels
# from recommender.online.recall import (
#     FollowingRecall,
#     CFRecall,
#     ContentRecall,
#     TrendingRecall
# )

# # Ranking
# from recommender.online.ranking import MLRanker, Reranker

# # Friend recommendation
# from recommender.online.friend_recommendation import FriendRecommendation

# from recommender.online.recall.cf_realtime import CFRealtimeRecall

# # Try optional imports
# try:
#     import redis
#     REDIS_AVAILABLE = True
# except ImportError:
#     REDIS_AVAILABLE = False
#     print("‚ö†Ô∏è  redis not available")

# try:
#     import lightgbm as lgb
#     LIGHTGBM_AVAILABLE = True
# except ImportError:
#     LIGHTGBM_AVAILABLE = False
#     print("‚ö†Ô∏è  lightgbm not available")

# # Setup logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='[%(asctime)s] %(levelname)s - %(name)s - %(message)s'
# )
# logger = logging.getLogger(__name__)

# @dataclass
# class RecallDiag:
#     following: int = 0
#     cf: int = 0
#     content: int = 0
#     trending: int = 0
#     total: int = 0
#     notes: Dict[str, str] = None  # optional human-readable notes


# class OnlineInferencePipeline:
#     """
#     Complete online inference pipeline
    
#     Features:
#     - Multi-channel recall (4 channels)
#     - ML ranking with LightGBM
#     - Business rules re-ranking
#     - Real-time user embedding updates
#     - Friend recommendations
#     - Redis caching
#     - < 200ms latency
#     """
    
#     def __init__(
#         self,
#         config_path: str = 'configs/config_online.yaml',
#         models_dir: str = 'models',
#         data_dir: str = 'dataset',
#         use_redis: bool = True
#     ):
#         """
#         Initialize online pipeline
        
#         Args:
#             config_path: Path to online config file
#             models_dir: Directory with model artifacts
#             data_dir: Directory with data (for development)
#             use_redis: Whether to use Redis cache
#         """
#         self.models_dir = Path(models_dir)
#         self.data_dir = Path(data_dir)
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 1: LOAD CONFIGURATION
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\n" + "="*70)
#         logger.info("INITIALIZING ONLINE INFERENCE PIPELINE")
#         logger.info("="*70)
        
#         logger.info("\nüìã Step 1: Loading configuration...")
#         with open(config_path, 'r', encoding='utf-8') as f:
#             self.config = yaml.safe_load(f)
        
#         self.recall_config = self.config.get('recall', {})
#         self.ranking_config = self.config.get('ranking', {})
#         self.reranking_config = self.config.get('reranking', {})
        
#         logger.info("‚úÖ Configuration loaded")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 2: CONNECT TO REDIS
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüîß Step 2: Connecting to Redis...")
        
#         self.redis = None
#         if use_redis and REDIS_AVAILABLE:
#             try:
#                 redis_config = self.config.get('redis', {})
#                 self.redis = redis.Redis(
#                     host=redis_config.get('host', 'localhost'),
#                     port=redis_config.get('port', 6380),
#                     db=redis_config.get('db', 0),
#                     decode_responses=False,
#                     socket_timeout=redis_config.get('socket_timeout', 5),
#                     max_connections=redis_config.get('max_connections', 50)
#                 )
#                 # Test connection
#                 self.redis.ping()
#                 logger.info("‚úÖ Redis connected")
#             except Exception as e:
#                 logger.warning(f"Redis connection failed: {e}")
#                 self.redis = None
#         else:
#             logger.warning("Redis disabled or not available")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 3: LOAD ARTIFACTS
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüì¶ Step 3: Loading model artifacts...")
        
#         self.artifact_mgr = ArtifactManager(base_dir=str(self.models_dir))
        
#         # Get version
#         model_version = self.config.get('models', {}).get('version', 'latest')
#         if model_version == 'latest':
#             self.current_version = self.artifact_mgr.get_latest_version()
#         else:
#             self.current_version = model_version
        
#         logger.info(f"Loading version: {self.current_version}")
        
#         # Load all artifacts
#         artifacts = self.artifact_mgr.load_artifacts(self.current_version)
        
#         # Extract components
#         self.embeddings = artifacts['embeddings']
#         self.faiss_index = artifacts.get('faiss_index')
#         self.faiss_post_ids = artifacts.get('faiss_post_ids', [])
#         self.cf_model = artifacts['cf_model']
#         self.ranking_model = artifacts['ranking_model']
#         self.ranking_scaler = artifacts['ranking_scaler']
#         self.ranking_feature_cols = artifacts['ranking_feature_cols']
#         self.user_stats = artifacts['user_stats']
#         self.author_stats = artifacts['author_stats']
#         self.following_dict = artifacts['following_dict']
#         self.metadata = artifacts.get('metadata', {})
        
#         logger.info("‚úÖ Artifacts loaded successfully!")
#         logger.info(f"   Post embeddings: {len(self.embeddings['post']):,}")
#         logger.info(f"   User embeddings: {len(self.embeddings['user']):,}")
#         logger.info(f"   Ranking features: {len(self.ranking_feature_cols)}")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 4: LOAD DATA (for development)
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüìä Step 4: Loading data...")
        
#         from recommender.common.data_loader import load_data
#         self.data = load_data(str(self.data_dir))
        
#         logger.info("‚úÖ Data loaded")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 5: INITIALIZE FEATURE ENGINEER
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüîß Step 5: Initializing feature engineer...")
        
#         self.feature_engineer = FeatureEngineer(
#             data=self.data,
#             user_stats=self.user_stats,
#             author_stats=self.author_stats,
#             following_dict=self.following_dict,
#             embeddings=self.embeddings
#         )
        
#         logger.info("‚úÖ Feature engineer initialized")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 6: INITIALIZE RECALL CHANNELS
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüîß Step 6: Initializing recall channels...")
        
#         channels_config = self.recall_config.get('channels', {})
        
#         # Channel 1: Following
#         self.following_recall = FollowingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             following_dict=self.following_dict,
#             config=channels_config.get('following', {})
#         )
        
#         # Channel 2: CF
#         self.cf_recall = CFRecall(
#             redis_client=self.redis,
#             data=self.data,
#             cf_model=self.cf_model,
#             config=channels_config.get('collaborative_filtering', {})
#         )
        
#         # Channel 3: Content
#         self.content_recall = ContentRecall(
#             redis_client=self.redis,
#             embeddings=self.embeddings,
#             faiss_index=self.faiss_index,
#             faiss_post_ids=self.faiss_post_ids,
#             data=self.data,
#             config=channels_config.get('content_based', {})
#         )
        
#         # Channel 4: Trending
#         self.trending_recall = TrendingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             config=channels_config.get('trending', {})
#         )
        
#         logger.info("‚úÖ Recall channels initialized")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 7: INITIALIZE RANKER & RERANKER
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüîß Step 7: Initializing ranker & reranker...")
        
#         # Ranker
#         self.ranker = MLRanker(
#             model=self.ranking_model,
#             scaler=self.ranking_scaler,
#             feature_cols=self.ranking_feature_cols,
#             feature_engineer=self.feature_engineer,
#             config=self.ranking_config
#         )
        
#         # Reranker
#         self.reranker = Reranker(config=self.reranking_config)
        
#         logger.info("‚úÖ Ranker & Reranker initialized")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # STEP 8: INITIALIZE FRIEND RECOMMENDATION
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         logger.info("\nüîß Step 8: Initializing friend recommendation...")
        
#         self.friend_recommender = FriendRecommendation(
#             data=self.data,
#             embeddings=self.embeddings,
#             cf_model=self.cf_model,
#             config=self.config.get('friend_recommendation', {})
#         )
        
#         logger.info("‚úÖ Friend recommendation initialized")
        
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#         # METRICS TRACKING
#         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
#         self.metrics = defaultdict(list)

#         self.cf_rt = None
#         rt_cfg = self.recall_config.get("channels", {}).get("cf_realtime", {})
#         if rt_cfg.get("enabled") and self.redis is not None:
#             self.cf_rt = CFRealtimeRecall(self.redis, rt_cfg)
        
#         logger.info("\n‚úÖ ONLINE PIPELINE READY!")
#         logger.info("="*70 + "\n")
    
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     # MAIN FEED GENERATION
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
#     def _build_post_metadata(self, post_ids: List[int]) -> Dict[int, Dict]:
#         """
#         Build metadata map {post_id: {...}} serving rerank:
#         - author_id: int | None
#         - created_at: datetime (UTC-naive) | None
#         - status: int | None
#         - content_hash: str | None
#         - title: str | None
#         - content: str | None
#         Safe with missing columns; automatically parse datetime.
#         """
#         meta: Dict[int, Dict] = {}
#         try:
#             post_df = self.data.get("post")
#             if post_df is None or post_df.empty:
#                 return meta

#             # Only get the necessary posts to reduce costs
#             df = post_df[post_df["Id"].isin(post_ids)] if "Id" in post_df.columns else post_df.copy()

#             # Define column names according to existing schema
#             col_id = "Id" if "Id" in df.columns else None
#             col_author = "UserId" if "UserId" in df.columns else None
#             col_created = "CreateDate" if "CreateDate" in df.columns else None
#             col_status = "Status" if "Status" in df.columns else None
#             col_hash = "ContentHash" if "ContentHash" in df.columns else None
#             col_title = "Title" if "Title" in df.columns else None
#             col_content = "Content" if "Content" in df.columns else None

#             if col_id is None:
#                 # No Id ‚áí cannot map metadata by post_id
#                 return meta

#             for _, r in df.iterrows():
#                 try:
#                     pid = int(r[col_id])
#                 except Exception:
#                     continue

#                 # author_id
#                 author_id = None
#                 if col_author and pd.notna(r.get(col_author)):
#                     try:
#                         author_id = int(r[col_author])
#                     except Exception:
#                         author_id = None

#                 # created_at (parse to UTC-naive)
#                 created_at = None
#                 if col_created and pd.notna(r.get(col_created)):
#                     try:
#                         dt = pd.to_datetime(r[col_created], utc=True)
#                         # to naive UTC for quick comparison
#                         created_at = dt.tz_convert(None) if hasattr(dt, "tz_convert") else dt.tz_localize(None)
#                     except Exception:
#                         created_at = None

#                 # status
#                 status = None
#                 if col_status and pd.notna(r.get(col_status)):
#                     try:
#                         status = int(r[col_status])
#                     except Exception:
#                         status = None

#                 # content hash / title / content
#                 content_hash = str(r[col_hash]) if col_hash and pd.notna(r.get(col_hash)) else None
#                 title = str(r[col_title]) if col_title and pd.notna(r.get(col_title)) else None
#                 content = str(r[col_content]) if col_content and pd.notna(r.get(col_content)) else None

#                 meta[pid] = {
#                     "author_id": author_id,
#                     "created_at": created_at,
#                     "status": status,
#                     "content_hash": content_hash,
#                     "title": title,
#                     "content": content,
#                 }

#         except Exception as e:
#             logger.warning(f"Build post_metadata failed: {e}", exc_info=True)

#         return meta

#     def _has_user_embedding(self, user_id: int) -> bool:
#         try:
#             return user_id in self.embeddings.get("user", {})
#         except Exception:
#             return False

#     def _user_following_count(self, user_id: int) -> int:
#         try:
#             flw = self.following_dict.get(user_id, [])
#             return len(flw)
#         except Exception:
#             return 0

#     def _trending_empty_last_hours(self, hours: int) -> bool:
#         try:
#             post_df = self.data.get("post")
#             if post_df is None or post_df.empty or "CreateDate" not in post_df.columns:
#                 return True
#             dt = pd.to_datetime(post_df["CreateDate"], errors="coerce", utc=True)
#             cutoff = pd.Timestamp.utcnow() - pd.Timedelta(hours=hours)
#             return bool((dt >= cutoff).sum() == 0)
#         except Exception:
#             return True

#     def _status_ok_mismatch_ratio(self, post_ids: List[int], ok_values: set) -> float:
#         """Return fraction of posts NOT in ok status among given ids."""
#         post_df = self.data.get("post")
#         if post_df is None or post_df.empty or "Id" not in post_df.columns or "Status" not in post_df.columns:
#             return 0.0
#         sub = post_df.loc[post_df["Id"].isin(post_ids), ["Id", "Status"]]
#         if sub.empty: 
#             return 0.0
#         bad = (~sub["Status"].isin(ok_values)).sum()
#         return round(bad / len(sub), 3)

#     # def generate_feed(
#     #     self,
#     #     user_id: int,
#     #     limit: int = 50,
#     #     exclude_seen: Optional[List[int]] = None
#     # ) -> List[Dict]:
#     #     """
#     #     Generate personalized feed for user
        
#     #     Pipeline:
#     #     1. Multi-channel recall (~1000 candidates)
#     #     2. Feature extraction (47 features √ó N posts)
#     #     3. ML ranking (LightGBM)
#     #     4. Re-ranking & business rules
#     #     5. Return top K
        
#     #     Args:
#     #         user_id: Target user ID
#     #         limit: Number of posts to return
#     #         exclude_seen: Post IDs to exclude
            
#     #     Returns:
#     #         List of post dicts with scores
#     #     """
#     #     start_time = time.time()
        
#     #     try:
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     #         # STAGE 1: MULTI-CHANNEL RECALL
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
#     #         t1 = time.time()
#     #         candidates = self._recall_candidates(user_id, target_count=1000)
#     #         recall_latency = (time.time() - t1) * 1000
#     #         self.metrics['recall_latency'].append(recall_latency)
            
#     #         if not candidates:
#     #             logger.warning(f"No candidates for user {user_id}")
#     #             return []
            
#     #         # Exclude seen posts
#     #         if exclude_seen:
#     #             candidates = [p for p in candidates if p not in exclude_seen]
            
#     #         logger.debug(f"Recall: {len(candidates)} candidates in {recall_latency:.1f}ms")
            
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     #         # STAGE 2: ML RANKING
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
#     #         t2 = time.time()
#     #         ranked_df = self.ranker.rank(user_id, candidates)
#     #         ranking_latency = (time.time() - t2) * 1000
#     #         self.metrics['ranking_latency'].append(ranking_latency)
            
#     #         if ranked_df.empty:
#     #             logger.warning(f"Ranking failed for user {user_id}")
#     #             return []
            
#     #         # Get top 100 for re-ranking
#     #         ranked_df = ranked_df.head(100)
#     #         print(ranked_df.head(10))
#     #         logger.debug(f"Ranking: {len(ranked_df)} posts in {ranking_latency:.1f}ms")
            
#     #         # === NEW: build post_metadata for top-N posts ===
#     #         top_post_ids = ranked_df["post_id"].astype(int).tolist()
#     #         post_metadata = self._build_post_metadata(top_post_ids)
#     #         if not post_metadata:
#     #             logger.debug("post_metadata is empty; freshness/diversity/dedup may be limited.")

#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     #         # STAGE 3: RE-RANKING
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
#     #         t3 = time.time()
#     #         final_feed = self.reranker.rerank(
#     #             ranked_df=ranked_df,
#     #             post_metadata=post_metadata,  # TODO: Add metadata
#     #             limit=limit
#     #         )
#     #         reranking_latency = (time.time() - t3) * 1000
#     #         self.metrics['reranking_latency'].append(reranking_latency)
            
#     #         logger.debug(f"Reranking: {len(final_feed)} posts in {reranking_latency:.1f}ms")
            
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     #         # METRICS
#     #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
#     #         total_latency = (time.time() - start_time) * 1000
#     #         self.metrics['total_latency'].append(total_latency)
            
#     #         logger.info(
#     #             f"Feed generated for user {user_id}: "
#     #             f"{len(final_feed)} posts | "
#     #             f"Latency: {total_latency:.1f}ms "
#     #             f"(R:{recall_latency:.0f} + M:{ranking_latency:.0f} + B:{reranking_latency:.0f})"
#     #         )
            
#     #         return final_feed
            
#     #     except Exception as e:
#     #         logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
#     #         return []
#     def generate_feed(self, user_id: int, limit: int = 50, exclude_seen: Optional[List[int]] = None) -> List[Dict]:
#         start_time = time.time()
#         try:
#             # 1) RECALL
#             t1 = time.time()
#             candidates, recall_diag = self._recall_candidates(user_id, target_count=self.recall_config.get("target_count", 1000))
#             self.metrics['recall_latency'].append((time.time() - t1) * 1000)

#             if not candidates:
#                 reasons = self._diagnose_no_candidates(user_id, recall_diag)
#                 logger.warning("No candidates for user %s | reasons=%s", user_id, reasons)
#                 # (tu·ª≥ ch·ªçn) l∆∞u reasons v√†o n∆°i API layer ƒë·ªçc ƒë·ªÉ tr·∫£ v·ªÅ JSON
#                 self._last_no_feed_reasons = reasons
#                 return []

#             # Exclude seen
#             exclude_set = set(exclude_seen) if exclude_seen else None
#             if exclude_set:
#                 candidates = [p for p in candidates if p not in exclude_set]

#             # 2) RANK
#             t2 = time.time()
#             ranked_df = self.ranker.rank(user_id, candidates)
#             self.metrics['ranking_latency'].append((time.time() - t2) * 1000)
#             if ranked_df.empty:
#                 self._last_no_feed_reasons = ["Ranking produced empty scores (check feature engineering/model inputs)"]
#                 return []

#             ranked_df = ranked_df.head(self.ranking_config.get('top_k', 100))

#             # 3) RERANK
#             top_ids = ranked_df["post_id"].astype(int).tolist()
#             post_metadata = self._build_post_metadata(top_ids)

#             # N·∫øu sau Quality/Dedup/Diversity r·ªóng, ch·∫©n ƒëo√°n ti·∫øp
#             t3 = time.time()
#             final_feed = self.reranker.rerank(ranked_df=ranked_df, post_metadata=post_metadata, limit=limit)
#             self.metrics['reranking_latency'].append((time.time() - t3) * 1000)

#             if not final_feed:
#                 reasons = self._diagnose_empty_after_rerank(top_ids, post_metadata)
#                 logger.warning("Empty after rerank for user %s | reasons=%s", user_id, reasons)
#                 self._last_no_feed_reasons = reasons
#                 return []

#             # OK
#             self._last_no_feed_reasons = []
#             return final_feed

#         except Exception as e:
#             logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
#             self._last_no_feed_reasons = [f"Unhandled exception: {type(e).__name__}"]
#             return []

    
#     def _recall_candidates(self, user_id: int, target_count: int = 1000) -> List[int]:
#         """
#         Multi-channel recall
        
#         Channels:
#         1. Following (400) - Posts from followed users
#         2. CF (300) - Posts liked by similar users
#         3. Content (200) - Similar posts via embeddings
#         4. Trending (100) - Popular posts
#         """
#         all_candidates = []

#         # Check acc
#         diag = RecallDiag(notes={})
        
#         # Channel 1: Following
#         # following_posts = self.following_recall.recall(user_id, k=400)
#         # Following
#         try:
#             following_posts = self.following_recall.recall(user_id, k=self.recall_config['channels']['following'].get('count', 400))
#         except Exception as e:
#             logger.warning(f"FollowingRecall failed: {e}")
#             following_posts = []
#         diag.following = len(following_posts); all_candidates.extend(following_posts)
#         print(f"Following posts: {following_posts}")
#         all_candidates.extend(following_posts)
        
#         # Channel 2: CF
#         # cf_posts = self.cf_recall.recall(user_id, k=300)
#         try:
#             cf_posts = self.cf_recall.recall(user_id, k=self.recall_config['channels']['collaborative_filtering'].get('count', 300))
#         except Exception as e:
#             logger.warning(f"CFRecall failed: {e}")
#             cf_posts = []
#         diag.cf = len(cf_posts); all_candidates.extend(cf_posts)
#         print(f"CF posts: {cf_posts}")
#         all_candidates.extend(cf_posts)
        
#         # Channel 3: Content
#         # content_posts = self.content_recall.recall(user_id, k=200)
#         try:
#             content_posts = self.content_recall.recall(user_id, k=self.recall_config['channels']['content_based'].get('count', 200))
#         except Exception as e:
#             logger.warning(f"ContentRecall failed: {e}")
#             content_posts = []
#         diag.content = len(content_posts); all_candidates.extend(content_posts)
#         print(f"Content posts: {content_posts}")
#         all_candidates.extend(content_posts)
        
#         # Channel 4: Trending
#         # trending_posts = self.trending_recall.recall(k=100)
#         try:
#             trending_posts = self.trending_recall.recall(k=self.recall_config['channels']['trending'].get('count', 100))
#         except Exception as e:
#             logger.warning(f"TrendingRecall failed: {e}")
#             trending_posts = []
#         diag.trending = len(trending_posts); all_candidates.extend(trending_posts)
#         print(f"Trending posts: {trending_posts}")
#         all_candidates.extend(trending_posts)
        
#         # Deduplicate (preserve order)
#         unique_candidates = list(dict.fromkeys(all_candidates))
#         diag.total = len(unique_candidates)
#         print(f"Unique candidates: {len(unique_candidates)}")

#         logger.info("RECALL SUMMARY | user=%s | following=%s cf=%s content=%s trending=%s | total=%s",
#                 user_id, diag.following, diag.cf, diag.content, diag.trending, diag.total)
        
#         flw_cnt = self._user_following_count(user_id)
#         if flw_cnt == 0:
#             diag.notes["following"] = "User has no following"

#         if self._trending_empty_last_hours(self.recall_config['channels']['trending'].get('trending_window_hours', 6)):
#             diag.notes["trending"] = "No posts within trending window"

#         if not self._has_user_embedding(user_id):
#             diag.notes["content"] = "User embedding missing (cold-start)"

#         if not unique_candidates:
#             logger.warning("No candidates after standard recall ‚Äî applying DEV fallback for user %s", user_id)
#             fallback = self._dev_backfill_candidates(user_id, target_count)
#             logger.info("DEV FALLBACK | user=%s | backfill=%s", user_id, len(fallback))
#             return fallback

#         return unique_candidates[:target_count], diag
    
#     def _dev_backfill_candidates(self, user_id: int, target_count: int = 1000) -> List[int]:
#         """
#         Fallback cho DEV/STAGING: n·ªõi l·ªèng ƒëi·ªÅu ki·ªán ƒë·ªÉ ƒë·∫£m b·∫£o c√≥ ·ª©ng vi√™n.
#         ∆Øu ti√™n: Trending r·∫•t r·ªông ‚Üí ContentRandom ‚Üí Random recent posts.
#         """
#         posts_df = self.data.get("post")
#         if posts_df is None or posts_df.empty:
#             return []

#         # 1) N·ªõi Trending (b·ªè min_engagement / c·ª≠a s·ªï r·ªông)
#         try:
#             loose_trending = self.trending_recall.recall(
#                 k=min(200, target_count),
#                 override_window_hours=720,          # 30 ng√†y
#                 override_min_engagement=0
#             )
#         except Exception:
#             loose_trending = []

#         # 2) Content random (n·∫øu c√≥ embeddings list)
#         try:
#             content_rand = self.content_recall.recall_random(k=min(300, target_count))
#         except Exception:
#             content_rand = []

#         # 3) Random recent posts (Status h·ª£p l·ªá n·∫øu c√≥)
#         try:
#             valid_status = {1, 10}   # DEV/PROD
#             df = posts_df.copy()
#             if "Status" in df.columns:
#                 df = df[df["Status"].isin(valid_status)]
#             if "CreateDate" in df.columns:
#                 df["__cd"] = pd.to_datetime(df["CreateDate"], errors="coerce")
#                 df = df.sort_values("__cd", ascending=False)
#             pool = df["Id"].astype(int).tolist()
#             recent_rand = pool[:min(500, len(pool))]
#         except Exception:
#             recent_rand = []

#         merged = list(dict.fromkeys(loose_trending + content_rand + recent_rand))
#         return merged[:target_count]

#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     # FRIEND RECOMMENDATION
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
#     def recommend_friends(
#         self,
#         user_id: int,
#         k: int = 20
#     ) -> List[Dict]:
#         """
#         Recommend potential friends
        
#         Args:
#             user_id: Target user ID
#             k: Number of recommendations
            
#         Returns:
#             List of friend recommendations
#         """
#         try:
#             recommendations = self.friend_recommender.recommend_friends(
#                 user_id=user_id,
#                 k=k
#             )
            
#             logger.info(f"Friend recommendations for user {user_id}: {len(recommendations)} users")
            
#             return recommendations
            
#         except Exception as e:
#             logger.error(f"Error recommending friends for user {user_id}: {e}", exc_info=True)
#             return []
    
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     # REAL-TIME USER EMBEDDING UPDATE
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
#     def update_user_embedding_realtime(
#         self,
#         user_id: int,
#         post_id: int,
#         action: str
#     ):
#         """
#         Update user embedding in real-time after interaction
        
#         Strategy: Incremental weighted average
#         new_embedding = (1 - Œ±) √ó old_embedding + Œ± √ó post_embedding
        
#         Args:
#             user_id: User ID
#             post_id: Post ID interacted with
#             action: Action type (like, comment, share, etc.)
#         """
#         # Check if real-time update enabled
#         if not self.config.get('user_embedding', {}).get('real_time_update', {}).get('enabled', False):
#             return
        
#         # Check if action triggers update
#         trigger_actions = self.config.get('user_embedding', {}).get('real_time_update', {}).get('trigger_actions', [])
#         if action not in trigger_actions:
#             return
        
#         try:
#             # Get post embedding
#             if post_id not in self.embeddings['post']:
#                 logger.debug(f"No embedding for post {post_id}")
#                 return
            
#             post_emb = self.embeddings['post'][post_id]
            
#             # Get current user embedding
#             if user_id in self.embeddings['user']:
#                 old_emb = self.embeddings['user'][user_id]
#             else:
#                 # First interaction - use post embedding
#                 old_emb = post_emb
            
#             # Learning rate (alpha)
#             alpha = self.config.get('user_embedding', {}).get('real_time_update', {}).get('incremental', {}).get('learning_rate', 0.1)
            
#             # Action weight
#             action_weights = {
#                 'like': 1.0,
#                 'comment': 1.5,
#                 'share': 2.0,
#                 'save': 1.2
#             }
#             action_weight = action_weights.get(action, 1.0)
            
#             # Weighted alpha
#             weighted_alpha = alpha * action_weight
            
#             # Update embedding (incremental weighted average)
#             new_emb = (1 - weighted_alpha) * old_emb + weighted_alpha * post_emb
            
#             # Normalize
#             new_emb = new_emb / (np.linalg.norm(new_emb) + 1e-8)
            
#             # Update in memory
#             self.embeddings['user'][user_id] = new_emb.astype(np.float32)
            
#             # Update Redis cache (async)
#             if self.redis is not None:
#                 try:
#                     key = f"user:{user_id}:embedding"
#                     value = new_emb.tobytes()
#                     self.redis.setex(key, 604800, value)  # 7 days TTL
#                 except Exception as e:
#                     logger.warning(f"Redis update failed: {e}")
            
#             logger.debug(f"Updated embedding for user {user_id} after {action} on post {post_id}")
            
#         except Exception as e:
#             logger.error(f"Error updating user embedding: {e}")
    
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#     # METRICS & MONITORING
#     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
#     def get_metrics(self) -> Dict:
#         """Get performance metrics"""
#         if not self.metrics['total_latency']:
#             return {
#                 'total_requests': 0,
#                 'avg_latency_ms': 0,
#                 'p50_latency_ms': 0,
#                 'p95_latency_ms': 0,
#                 'p99_latency_ms': 0
#             }
        
#         latencies = self.metrics['total_latency']
        
#         return {
#             'total_requests': len(latencies),
#             'avg_latency_ms': np.mean(latencies),
#             'p50_latency_ms': np.percentile(latencies, 50),
#             'p95_latency_ms': np.percentile(latencies, 95),
#             'p99_latency_ms': np.percentile(latencies, 99),
#             'avg_recall_latency_ms': np.mean(self.metrics['recall_latency']),
#             'avg_ranking_latency_ms': np.mean(self.metrics['ranking_latency']),
#             'avg_reranking_latency_ms': np.mean(self.metrics['reranking_latency'])
#         }
    
#     def _diagnose_no_candidates(self, user_id: int, diag: RecallDiag) -> List[str]:
#         reasons = []
#         # Following
#         if diag.following == 0:
#             flw_cnt = self._user_following_count(user_id)
#             if flw_cnt == 0:
#                 reasons.append("No following (FollowingRecall empty)")
#             else:
#                 reasons.append("FollowingRecall returned 0 (filtering or time-window?)")

#         # Trending
#         if diag.trending == 0:
#             reasons.append("Trending empty (no posts within trending window)")

#         # CF
#         if diag.cf == 0:
#             reasons.append("CF has no similar users/items (no interactions/latent)")

#         # Content
#         if diag.content == 0:
#             if not self._has_user_embedding(user_id):
#                 reasons.append("Content cold-start (no user embedding)")
#             else:
#                 reasons.append("Content recall returned 0 (index/filter?)")

#         if not reasons:
#             reasons.append("All recall channels returned 0 due to strict filters")
#         return reasons

#     def _diagnose_empty_after_rerank(self, post_ids: List[int], post_metadata: Dict[int, Dict]) -> List[str]:
#         reasons = []
#         # Status mismatch check
#         ok_values = set()
#         ok_v = self.reranking_config.get("quality_status_ok_value", 1)
#         ok_values.add(int(ok_v))
#         for v in self.reranking_config.get("quality_status_ok_values", []):
#             try: ok_values.add(int(v))
#             except: pass

#         mismatch_ratio = self._status_ok_mismatch_ratio(post_ids, ok_values)
#         if mismatch_ratio >= 0.9:  # g·∫ßn nh∆∞ t·∫•t c·∫£ b·ªã lo·∫°i b·ªüi Status
#             reasons.append(f"Quality filter removed ~{int(mismatch_ratio*100)}% due to Status not in {sorted(ok_values)}")

#         # Freshness: kh√¥ng boost ‚Üí kh√¥ng ph·∫£i l√Ω do r·ªóng, ch·ªâ l√† 0-boost
#         # Dedup: n·∫øu c√≥ content_hash tr√πng nhi·ªÅu, c√≥ th·ªÉ g√¢y r·ªóng (hi·∫øm)
#         # Diversity: hi·∫øm khi ƒÉn s·∫°ch, tr·ª´ khi top-K to√†n m·ªôt t√°c gi·∫£ v·ªõi cap total=0 (kh√¥ng h·ª£p l√Ω)

#         if not reasons:
#             reasons.append("Post-filtering removed all candidates (check Status mapping / dedup / diversity caps)")
#         return reasons

#     def print_metrics(self):
#         """Print performance metrics"""
#         metrics = self.get_metrics()
        
#         print("\n" + "="*70)
#         print("PERFORMANCE METRICS")
#         print("="*70)
#         print(f"Total requests: {metrics['total_requests']}")
#         print(f"\nLatency:")
#         print(f"  Avg: {metrics['avg_latency_ms']:.1f}ms")
#         print(f"  P50: {metrics['p50_latency_ms']:.1f}ms")
#         print(f"  P95: {metrics['p95_latency_ms']:.1f}ms")
#         print(f"  P99: {metrics['p99_latency_ms']:.1f}ms")
#         print(f"\nStage breakdown:")
#         print(f"  Recall: {metrics['avg_recall_latency_ms']:.1f}ms")
#         print(f"  Ranking: {metrics['avg_ranking_latency_ms']:.1f}ms")
#         print(f"  Reranking: {metrics['avg_reranking_latency_ms']:.1f}ms")
#         print("="*70 + "\n")


# # ============================================================================
# # EXAMPLE USAGE
# # ============================================================================

# if __name__ == "__main__":
#     """Test online pipeline"""
    
#     # Initialize
#     print("Initializing pipeline...")
#     pipeline = OnlineInferencePipeline(
#         config_path='configs/config_online.yaml',
#         models_dir='models',
#         data_dir='dataset',
#         use_redis=False  # Disable Redis for testing
#     )
    
#     # Test feed generation
#     print("\nTesting feed generation...")
#     feed = pipeline.generate_feed(user_id=1, limit=50)
    
#     print(f"\nGenerated feed: {len(feed)} posts")
#     print(f"Sample: {feed[:5]}")
    
#     # Test friend recommendation
#     print("\nTesting friend recommendation...")
#     friends = pipeline.recommend_friends(user_id=1, k=20)
    
#     print(f"\nRecommended friends: {len(friends)}")
#     print(f"Sample: {friends[:5]}")
    
#     # Print metrics
#     pipeline.print_metrics()



# # """
# # ONLINE INFERENCE PIPELINE - INTEGRATED WITH NEW OFFLINE ARTIFACTS
# # ==================================================================
# # Complete online serving pipeline using versioned artifacts

# # Architecture:
# # - Load models from ArtifactManager (latest version)
# # - Multi-channel recall (Following, CF, Content, Trending)
# # - Feature extraction (47 features)
# # - ML Ranking (LightGBM)
# # - Re-ranking & Business rules

# # Target: < 200ms end-to-end latency
# # """

# # import os
# # import sys
# # import time
# # import numpy as np
# # import pandas as pd
# # from typing import List, Dict, Optional, Tuple
# # from datetime import datetime, timedelta
# # from pathlib import Path
# # import logging
# # import warnings

# # warnings.filterwarnings('ignore')

# # # Add project root
# # sys.path.append(str(Path(__file__).parent.parent.parent))

# # # Try imports
# # try:
# #     from recommender.offline.artifact_manager import ArtifactManager
# #     ARTIFACT_MANAGER_AVAILABLE = True
# # except ImportError:
# #     ARTIFACT_MANAGER_AVAILABLE = False
# #     print("‚ö†Ô∏è  ArtifactManager not available")

# # try:
# #     from recommender.common.feature_engineer import FeatureEngineer
# #     FEATURE_ENGINEER_AVAILABLE = True
# # except ImportError:
# #     FEATURE_ENGINEER_AVAILABLE = False
# #     print("‚ö†Ô∏è  FeatureEngineer not available")

# # try:
# #     import redis
# #     REDIS_AVAILABLE = True
# # except ImportError:
# #     REDIS_AVAILABLE = False
# #     print("‚ö†Ô∏è  redis not available. Install: pip install redis")

# # try:
# #     import lightgbm as lgb
# #     LIGHTGBM_AVAILABLE = True
# # except ImportError:
# #     LIGHTGBM_AVAILABLE = False
# #     print("‚ö†Ô∏è  lightgbm not available")

# # try:
# #     import faiss
# #     FAISS_AVAILABLE = True
# # except ImportError:
# #     FAISS_AVAILABLE = False
# #     print("‚ö†Ô∏è  faiss not available")


# # from recommender.online.recall.following_recall import FollowingRecall
# # from recommender.online.recall.base_recall import BaseRecall


# # # Setup logging
# # logging.basicConfig(
# #     level=logging.INFO,
# #     format='[%(asctime)s] %(levelname)s - %(message)s',
# #     datefmt='%Y-%m-%d %H:%M:%S'
# # )
# # logger = logging.getLogger(__name__)


# # class OnlineInferencePipeline:
# #     """
# #     Complete online inference pipeline
    
# #     Features:
# #     - Load from versioned artifacts (ArtifactManager)
# #     - Multi-channel recall
# #     - ML ranking with LightGBM
# #     - Redis caching
# #     - < 200ms latency
# #     """
    
# #     def __init__(
# #         self,
# #         models_dir: str = 'models',
# #         redis_host: str = 'localhost',
# #         redis_port: int = 6380,
# #         redis_db: int = 0,
# #         use_redis: bool = True
# #     ):
# #         """
# #         Initialize online pipeline
        
# #         Args:
# #             models_dir: Directory with model artifacts
# #             redis_host: Redis host
# #             redis_port: Redis port
# #             redis_db: Redis database number
# #             use_redis: Enable Redis caching
# #         """
# #         logger.info("="*70)
# #         logger.info("INITIALIZING ONLINE INFERENCE PIPELINE")
# #         logger.info("="*70)
        
# #         self.models_dir = Path(models_dir)
# #         self.use_redis = use_redis and REDIS_AVAILABLE
        
# #         # Performance metrics
# #         self.metrics = {
# #             'recall_latency': [],
# #             'feature_latency': [],
# #             'ranking_latency': [],
# #             'reranking_latency': [],
# #             'total_latency': []
# #         }
        
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #         # STEP 1: CONNECT TO REDIS
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
# #         if self.use_redis:
# #             try:
# #                 self.redis_client = redis.Redis(
# #                     host=redis_host,
# #                     port=redis_port,
# #                     db=redis_db,
# #                     decode_responses=False  # For binary data
# #                 )
# #                 # Test connection
# #                 self.redis_client.ping()
# #                 logger.info(f"‚úÖ Connected to Redis: {redis_host}:{redis_port}/{redis_db}")
# #             except Exception as e:
# #                 logger.warning(f"‚ö†Ô∏è  Redis connection failed: {e}")
# #                 logger.warning("‚ö†Ô∏è  Falling back to no-cache mode")
# #                 self.use_redis = False
# #                 self.redis_client = None
# #         else:
# #             self.redis_client = None
# #             logger.info("‚ÑπÔ∏è  Redis disabled (using no-cache mode)")
        
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #         # STEP 2: LOAD ARTIFACTS FROM LATEST VERSION
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
# #         if not ARTIFACT_MANAGER_AVAILABLE:
# #             raise ImportError("ArtifactManager required")
        
# #         logger.info("\nüì¶ Loading artifacts...")
        
# #         self.artifact_mgr = ArtifactManager(base_dir=self.models_dir)
        
# #         # Get latest version
# #         try:
# #             self.current_version = self.artifact_mgr.get_latest_version()
# #             logger.info(f"   Latest version: {self.current_version}")
# #         except FileNotFoundError:
# #             raise FileNotFoundError(
# #                 f"No models found in {self.models_dir}. "
# #                 "Run offline training first: python scripts/offline/main_offline_pipeline.py"
# #             )
        
# #         # Load all artifacts
# #         artifacts = self.artifact_mgr.load_artifacts(self.current_version)
        
# #         # Extract components
# #         self.embeddings = artifacts['embeddings']
# #         self.faiss_index = artifacts['faiss_index']
# #         self.faiss_post_ids = artifacts['faiss_post_ids']
# #         self.cf_model = artifacts['cf_model']
# #         self.ranking_model = artifacts['ranking_model']
# #         self.ranking_scaler = artifacts['ranking_scaler']
# #         self.ranking_feature_cols = artifacts['ranking_feature_cols']
# #         self.user_stats = artifacts['user_stats']
# #         self.author_stats = artifacts['author_stats']
# #         self.following_dict = artifacts['following_dict']
# #         self.metadata = artifacts['metadata']
        
# #         logger.info("‚úÖ Artifacts loaded successfully!")
# #         logger.info(f"   Post embeddings: {len(self.embeddings['post']):,}")
# #         logger.info(f"   User embeddings: {len(self.embeddings['user']):,}")
# #         logger.info(f"   CF users: {len(self.cf_model['user_ids']):,}")
# #         logger.info(f"   Faiss index: {self.faiss_index.ntotal:,} vectors")
# #         logger.info(f"   Ranking features: {len(self.ranking_feature_cols)}")
        
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #         # STEP 3: INITIALIZE FEATURE ENGINEER
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
# #         if not FEATURE_ENGINEER_AVAILABLE:
# #             raise ImportError("FeatureEngineer required")
        
# #         # Load data (dummy for online - only need structure)
# #         # In production, this would connect to live database
# #         logger.info("\nüîß Initializing feature engineer...")
        
# #         # Create dummy data structure (will be replaced by Redis/DB in production)
# #         self.data = {
# #             'user': pd.DataFrame(),
# #             'post': pd.DataFrame(),
# #             'postreaction': pd.DataFrame(),
# #             'friendship': pd.DataFrame()
# #         }
        
# #         self.feature_engineer = FeatureEngineer(
# #             data=self.data,
# #             user_stats=self.user_stats,
# #             author_stats=self.author_stats,
# #             following_dict=self.following_dict,
# #             embeddings=self.embeddings
# #         )
        
# #         logger.info("‚úÖ Feature engineer initialized")
        
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #         # COMPLETE
# #         # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        
# #         logger.info("\n‚úÖ ONLINE PIPELINE READY!")
# #         logger.info("="*70 + "\n")
    
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #     # MAIN INFERENCE METHOD
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
# #     def get_feed(
# #         self,
# #         user_id: int,
# #         limit: int = 50,
# #         exclude_seen: Optional[set] = None
# #     ) -> List[Dict]:
# #         """
# #         Get personalized feed for user
        
# #         Args:
# #             user_id: Target user ID
# #             limit: Number of posts to return
# #             exclude_seen: Set of post IDs to exclude
        
# #         Returns:
# #             List of dicts with post_id, score, metadata
# #         """
# #         start_time = time.time()
        
# #         try:
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #             # STAGE 1: MULTI-CHANNEL RECALL (~1000 candidates)
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
# #             t1 = time.time()
# #             candidates = self._recall_candidates(user_id, target_count=1000)
# #             recall_latency = (time.time() - t1) * 1000
# #             self.metrics['recall_latency'].append(recall_latency)
            
# #             if not candidates:
# #                 logger.warning(f"No candidates found for user {user_id}")
# #                 return []
            
# #             # Filter out seen posts
# #             if exclude_seen:
# #                 candidates = [p for p in candidates if p not in exclude_seen]
            
# #             logger.debug(f"Recall: {len(candidates)} candidates in {recall_latency:.1f}ms")
            
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #             # STAGE 2: FEATURE EXTRACTION (47 features √ó N posts)
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
# #             t2 = time.time()
# #             features_df = self._extract_features_batch(user_id, candidates)
# #             feature_latency = (time.time() - t2) * 1000
# #             self.metrics['feature_latency'].append(feature_latency)
            
# #             if features_df.empty:
# #                 logger.warning(f"No features extracted for user {user_id}")
# #                 return []
            
# #             logger.debug(f"Features: {len(features_df)} posts in {feature_latency:.1f}ms")
            
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #             # STAGE 3: ML RANKING (LightGBM)
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
# #             t3 = time.time()
# #             features_df = self._rank_candidates(features_df)
# #             ranking_latency = (time.time() - t3) * 1000
# #             self.metrics['ranking_latency'].append(ranking_latency)
            
# #             # Get top 100 for re-ranking
# #             features_df = features_df.nlargest(100, 'ml_score')
            
# #             logger.debug(f"Ranking: {len(features_df)} posts in {ranking_latency:.1f}ms")
            
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #             # STAGE 4: RE-RANKING & BUSINESS RULES
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
# #             t4 = time.time()
# #             final_feed = self._rerank_with_business_rules(features_df, limit)
# #             reranking_latency = (time.time() - t4) * 1000
# #             self.metrics['reranking_latency'].append(reranking_latency)
            
# #             logger.debug(f"Reranking: {len(final_feed)} posts in {reranking_latency:.1f}ms")
            
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #             # METRICS
# #             # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            
# #             total_latency = (time.time() - start_time) * 1000
# #             self.metrics['total_latency'].append(total_latency)
            
# #             logger.info(
# #                 f"Feed generated for user {user_id}: "
# #                 f"{len(final_feed)} posts | "
# #                 f"Latency: {total_latency:.1f}ms "
# #                 f"(R:{recall_latency:.0f} + F:{feature_latency:.0f} + "
# #                 f"M:{ranking_latency:.0f} + B:{reranking_latency:.0f})"
# #             )
            
# #             return final_feed
            
# #         except Exception as e:
# #             logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
# #             return []
    
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #     # STAGE 1: MULTI-CHANNEL RECALL
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
# #     def _recall_candidates(self, user_id: int, target_count: int = 1000) -> List[int]:
# #         """
# #         Multi-channel recall
        
# #         Channels:
# #         1. Following (400) - Posts from followed users
# #         2. CF (300) - Posts liked by similar users
# #         3. Content (200) - Similar posts via embeddings
# #         4. Trending (100) - Popular posts
        
# #         Returns:
# #             List of unique post IDs
# #         """
# #         all_candidates = []
        
# #         # Channel 1: Following
# #         following_posts = self._recall_following(user_id, k=400)
# #         all_candidates.extend(following_posts)
        
# #         # Channel 2: Collaborative Filtering
# #         cf_posts = self._recall_cf(user_id, k=300)
# #         all_candidates.extend(cf_posts)
        
# #         # Channel 3: Content-based (Embeddings)
# #         content_posts = self._recall_content(user_id, k=200)
# #         all_candidates.extend(content_posts)
        
# #         # Channel 4: Trending
# #         trending_posts = self._recall_trending(k=100)
# #         all_candidates.extend(trending_posts)
        
# #         # Deduplicate (preserve order)
# #         unique_candidates = list(dict.fromkeys(all_candidates))
        
# #         return unique_candidates[:target_count]
    
# #     def _recall_following(self, user_id: int, k: int) -> List[int]:
# #         """Recall posts from followed users"""
# #         # Get followed users
# #         followed_users = self.following_dict.get(user_id, [])
        
# #         if not followed_users:
# #             return []
        
# #         # In production: Query Redis sorted set
# #         # following:{user_id}:posts (sorted by timestamp)
        
# #         # For now: Return empty (needs live data)
# #         return []
    
# #     def _recall_cf(self, user_id: int, k: int) -> List[int]:
# #         """Recall via collaborative filtering"""
# #         # Get similar users
# #         user_similarities = self.cf_model['user_similarities']
# #         similar_users = user_similarities.get(user_id, [])
        
# #         if not similar_users:
# #             return []
        
# #         # Get posts liked by similar users
# #         # In production: Query from Redis or database
        
# #         # For now: Return top K most similar items
# #         item_similarities = self.cf_model.get('item_similarities', {})
        
# #         # Return first K items (placeholder)
# #         all_items = list(item_similarities.keys())[:k] if item_similarities else []
        
# #         return all_items
    
# #     def _recall_content(self, user_id: int, k: int) -> List[int]:
# #         """Recall via content similarity (embeddings)"""
# #         # Get user embedding
# #         user_emb = self.embeddings['user'].get(user_id)
        
# #         if user_emb is None or not FAISS_AVAILABLE:
# #             return []
        
# #         # Normalize
# #         user_emb = user_emb / (np.linalg.norm(user_emb) + 1e-8)
        
# #         # Search Faiss
# #         distances, indices = self.faiss_index.search(
# #             user_emb.reshape(1, -1).astype(np.float32),
# #             k * 2  # Over-fetch
# #         )
        
# #         # Convert indices to post IDs
# #         post_ids = [int(self.faiss_post_ids[idx]) for idx in indices[0]]
        
# #         return post_ids[:k]
    
# #     def _recall_trending(self, k: int) -> List[int]:
# #         """Recall trending posts"""
# #         # In production: Query Redis sorted set
# #         # trending:global:6h (sorted by engagement score)
        
# #         # For now: Return empty
# #         return []
    
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #     # STAGE 2: FEATURE EXTRACTION
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
# #     def _extract_features_batch(
# #         self,
# #         user_id: int,
# #         post_ids: List[int]
# #     ) -> pd.DataFrame:
# #         """Extract 47 features for each (user, post) pair"""
# #         features_list = []
# #         timestamp = datetime.now()
        
# #         for post_id in post_ids:
# #             try:
# #                 features = self.feature_engineer.extract_features(
# #                     user_id,
# #                     post_id,
# #                     timestamp
# #                 )
# #                 features['post_id'] = post_id
# #                 features_list.append(features)
# #             except Exception as e:
# #                 logger.debug(f"Feature extraction failed for post {post_id}: {e}")
# #                 continue
        
# #         if not features_list:
# #             return pd.DataFrame()
        
# #         features_df = pd.DataFrame(features_list)
        
# #         # Ensure all features present
# #         for col in self.ranking_feature_cols:
# #             if col not in features_df.columns:
# #                 features_df[col] = 0.0
        
# #         return features_df
    
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #     # STAGE 3: ML RANKING
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
# #     def _rank_candidates(self, features_df: pd.DataFrame) -> pd.DataFrame:
# #         """Rank using LightGBM model"""
# #         # Select features
# #         X = features_df[self.ranking_feature_cols].fillna(0)
        
# #         # Scale
# #         X_scaled = self.ranking_scaler.transform(X)
        
# #         # Predict
# #         scores = self.ranking_model.predict(X_scaled)
        
# #         # Add to dataframe
# #         features_df['ml_score'] = scores
        
# #         return features_df
    
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #     # STAGE 4: RE-RANKING
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
# #     def _rerank_with_business_rules(
# #         self,
# #         ranked_df: pd.DataFrame,
# #         limit: int
# #     ) -> List[Dict]:
# #         """Apply business rules and diversity"""
# #         # Sort by ML score
# #         ranked_df = ranked_df.sort_values('ml_score', ascending=False)
        
# #         final_feed = []
# #         last_author_id = None
# #         same_author_count = 0
        
# #         for _, row in ranked_df.iterrows():
# #             post_id = int(row['post_id'])
# #             score = float(row['ml_score'])
            
# #             # Get author (from embeddings or stats)
# #             # Placeholder: author_id = post_id % 1000
# #             author_id = post_id % 1000
            
# #             # Rule 1: Diversity (max 2 consecutive from same author)
# #             if author_id == last_author_id:
# #                 same_author_count += 1
# #                 if same_author_count >= 2:
# #                     continue
# #             else:
# #                 same_author_count = 0
# #                 last_author_id = author_id
            
# #             # Rule 2: Freshness boost (would need post creation time)
# #             # Placeholder: No boost
            
# #             final_feed.append({
# #                 'post_id': post_id,
# #                 'score': score,
# #                 'author_id': author_id
# #             })
            
# #             if len(final_feed) >= limit:
# #                 break
        
# #         return final_feed
    
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# #     # UTILITIES
# #     # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
# #     def get_metrics_summary(self) -> Dict:
# #         """Get performance metrics summary"""
# #         if not self.metrics['total_latency']:
# #             return {}
        
# #         summary = {}
# #         for key, values in self.metrics.items():
# #             if values:
# #                 summary[key] = {
# #                     'mean': np.mean(values),
# #                     'p50': np.percentile(values, 50),
# #                     'p95': np.percentile(values, 95),
# #                     'p99': np.percentile(values, 99),
# #                     'min': np.min(values),
# #                     'max': np.max(values)
# #                 }
        
# #         return summary
    
# #     def print_metrics(self):
# #         """Print performance metrics"""
# #         summary = self.get_metrics_summary()
        
# #         print("\n" + "="*70)
# #         print("ONLINE PIPELINE PERFORMANCE METRICS")
# #         print("="*70)
        
# #         for metric, stats in summary.items():
# #             print(f"\n{metric}:")
# #             print(f"  Mean:  {stats['mean']:.1f}ms")
# #             print(f"  P50:   {stats['p50']:.1f}ms")
# #             print(f"  P95:   {stats['p95']:.1f}ms")
# #             print(f"  P99:   {stats['p99']:.1f}ms")
# #             print(f"  Range: [{stats['min']:.1f}, {stats['max']:.1f}]ms")
        
# #         print("="*70 + "\n")


# # # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# # # MAIN TESTING
# # # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# # if __name__ == "__main__":
# #     """
# #     Test online inference pipeline
# #     """
# #     # Initialize pipeline
# #     pipeline = OnlineInferencePipeline(
# #         models_dir='models',
# #         use_redis=False  # Disable Redis for testing
# #     )
    
# #     # Test with a few users
# #     test_user_ids = [1, 2, 3, 4, 5]
    
# #     print("\n" + "="*70)
# #     print("TESTING INFERENCE")
# #     print("="*70 + "\n")
    
# #     for user_id in test_user_ids:
# #         feed = pipeline.get_feed(user_id, limit=50)
# #         print(f"User {user_id}: Generated {len(feed)} posts")
    
# #     # Print metrics
# #     pipeline.print_metrics()



# """
# ONLINE INFERENCE PIPELINE - PRODUCTION READY
# ============================================
# Flow: Recall ‚Üí Feature Extraction ‚Üí Ranking ‚Üí Re-ranking ‚Üí Output
# """

# from __future__ import annotations
# import os
# import sys
# import time
# import logging
# import warnings
# from pathlib import Path
# from typing import List, Dict, Optional, Tuple
# from dataclasses import dataclass
# from collections import defaultdict

# import numpy as np
# import pandas as pd
# import yaml
# from datetime import datetime

# # Add project root
# sys.path.append(str(Path(__file__).parent.parent.parent))

# warnings.filterwarnings('ignore')
# logger = logging.getLogger(__name__)

# # Core imports
# from recommender.offline.artifact_manager import ArtifactManager
# from recommender.common.feature_engineer import FeatureEngineer

# # Recall channels
# from recommender.online.recall import (
#     FollowingRecall,
#     CFRecall,
#     ContentRecall,
#     TrendingRecall
# )

# # Ranking
# from recommender.online.ranking import MLRanker, Reranker

# # Optional caches
# try:
#     import redis
#     REDIS_AVAILABLE = True
# except ImportError:
#     REDIS_AVAILABLE = False
#     logger.warning("redis not available")

# # SQLAlchemy (backend DB optional)
# from sqlalchemy import create_engine, text
# from sqlalchemy.orm import sessionmaker
# from recommender.online.realtime_handlers import RealtimeHandlers
# from recommender.online.realtime_jobs import RealTimeJobs
# from recommender.online.recall.covisit import CovisitRecall

# @dataclass
# class RecallDiag:
#     following: int = 0
#     cf: int = 0
#     content: int = 0
#     trending: int = 0
#     total: int = 0
#     notes: Dict[str, str] = None


# class OnlineInferencePipeline:
#     """
#     Complete online inference pipeline

#     Features:
#     - Multi-channel recall (4 channels)
#     - ML ranking with LightGBM
#     - Business rules re-ranking
#     - Real-time user embedding updates
#     - Redis caching
#     """

#     def __init__(
#         self,
#         config_path: str = 'configs/config_online.yaml',
#         models_dir: str = 'models',
#         data_dir: str = 'dataset',
#         use_redis: bool = True
#     ):
#         self.models_dir = Path(models_dir)
#         self.data_dir = Path(data_dir)

#         # ============== STEP 1: CONFIG ==========================
#         logger.info("\n" + "="*70)
#         logger.info("INITIALIZING ONLINE INFERENCE PIPELINE")
#         logger.info("="*70)

#         with open(config_path, 'r', encoding='utf-8') as f:
#             self.config = yaml.safe_load(f)

#         self.recall_config = self.config.get('recall', {})
#         self.ranking_config = self.config.get('ranking', {})
#         self.reranking_config = self.config.get('reranking', {})

#         # ============== STEP 2: REDIS ===========================
#         self.redis = None
#         if use_redis and REDIS_AVAILABLE:
#             try:
#                 rc = self.config.get('redis', {})
#                 self.redis = redis.Redis(
#                     host=rc.get('host', 'localhost'),
#                     port=rc.get('port', 6381),
#                     db=rc.get('db', 0),
#                     decode_responses=False,
#                     socket_timeout=rc.get('socket_timeout', 5),
#                     max_connections=rc.get('max_connections', 50)
#                 )
#                 self.redis.ping()
#                 logger.info("‚úÖ Redis connected")
#             except Exception as e:
#                 logger.warning(f"Redis connection failed: {e}")
#                 self.redis = None
#         else:
#             logger.warning("Redis disabled or not available")

#         # ============== STEP 2b: BACKEND DB (MySQL optional) ====
#         self.db_engine = None
#         self.db_session_factory = None
#         try:
#             be = self.config.get('backend_db', {}) or {}
#             if be.get('enabled'):
#                 connect_args = be.get('connect_args', {}) or {}
#                 self.db_engine = create_engine(
#                     be['url'],
#                     pool_size=be.get('pool_size', 20),
#                     max_overflow=be.get('max_overflow', 40),
#                     pool_recycle=be.get('pool_recycle', 1800),
#                     pool_pre_ping=be.get('pool_pre_ping', True),
#                     connect_args=connect_args,           # üëà NEW
#                     future=True,
#                 )
#                 self.db_session_factory = sessionmaker(
#                     bind=self.db_engine, autocommit=False, autoflush=False, future=True
#                 )
#                 # Ping ngay v√† log chi ti·∫øt n·∫øu fail
#                 try:
#                     with self.db_engine.connect() as conn:
#                         conn.execute(text("SELECT 1"))
#                     logger.info("‚úÖ Backend DB connected")
#                 except Exception as e:
#                     logger.warning("Backend DB initial ping failed: %s", e)  # üëà NEW
#             else:
#                 logger.info("Backend DB disabled in config")
#         except Exception as e:
#             logger.warning(f"Backend DB connection failed: {e}")
#             self.db_engine = None
#             self.db_session_factory = None

#         # ============== STEP 3: ARTIFACTS =======================
#         logger.info("\nüì¶ Loading model artifacts...")
#         self.artifact_mgr = ArtifactManager(base_dir=str(self.models_dir))

#         model_version = self.config.get('models', {}).get('version', 'latest')
#         self.current_version = self.artifact_mgr.get_latest_version() if model_version == 'latest' else model_version

#         artifacts = self.artifact_mgr.load_artifacts(self.current_version)

#         self.embeddings = artifacts['embeddings']
#         self.faiss_index = artifacts.get('faiss_index')
#         self.faiss_post_ids = artifacts.get('faiss_post_ids', [])
#         self.cf_model = artifacts['cf_model']
#         self.ranking_model = artifacts['ranking_model']
#         self.ranking_scaler = artifacts['ranking_scaler']
#         self.ranking_feature_cols = artifacts['ranking_feature_cols']
#         self.user_stats = artifacts['user_stats']
#         self.author_stats = artifacts['author_stats']
#         self.following_dict = artifacts['following_dict']
#         self.metadata = artifacts.get('metadata', {})

#         logger.info("‚úÖ Artifacts loaded")

#         # ============== STEP 4: DATA (for dev) ==================
#         logger.info("\nüìä Loading data...")
#         from recommender.common.data_loader import load_data
#         self.data = load_data(str(self.data_dir))
#         logger.info("‚úÖ Data loaded")

#         # ============== STEP 5: FEATURE ENGINEER ================
#         self.feature_engineer = FeatureEngineer(
#             data=self.data,
#             user_stats=self.user_stats,
#             author_stats=self.author_stats,
#             following_dict=self.following_dict,
#             embeddings=self.embeddings,
#             redis_client=self.redis
#         )

#         # ============== STEP 6: RECALL CHANNELS =================
#         channels_config = self.recall_config.get('channels', {})
#         self.following_recall = FollowingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             following_dict=self.following_dict,
#             config=channels_config.get('following', {})
#         )
#         self.cf_recall = CFRecall(
#             redis_client=self.redis,
#             data=self.data,
#             cf_model=self.cf_model,
#             config=channels_config.get('collaborative_filtering', {})
#         )
#         self.content_recall = ContentRecall(
#             redis_client=self.redis,
#             embeddings=self.embeddings,
#             faiss_index=self.faiss_index,
#             faiss_post_ids=self.faiss_post_ids,
#             data=self.data,
#             config=channels_config.get('content_based', {})
#         )
#         self.trending_recall = TrendingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             config=channels_config.get('trending', {})
#         )

#         # ============== STEP 7: RANKER & RERANKER ===============
#         self.ranker = MLRanker(
#             model=self.ranking_model,
#             scaler=self.ranking_scaler,
#             feature_cols=self.ranking_feature_cols,
#             feature_engineer=self.feature_engineer,
#             config=self.ranking_config
#         )
#         self.reranker = Reranker(config=self.reranking_config)

#         # ============== METRICS =================================
#         self.metrics = defaultdict(list)
#         self._last_recall_diag: Optional[RecallDiag] = None
#         self._last_no_feed_reasons: List[str] = []


#         # Realtime helpers
#         self.rt_handlers = RealtimeHandlers(self.redis, action_weights=self.config.get("user_embedding", {}).get("real_time_update", {}).get("action_weights", None))
#         self.rt_jobs = RealTimeJobs(self.redis,
#             interval_trending= self.config.get("recall", {}).get("channels", {}).get("trending", {}).get("refresh_interval_seconds", 300),
#             interval_post_feat=900
#         )
#         self.rt_jobs.start()

#         # Covisit recall (optional)
#         self.covisit_recall = CovisitRecall(self.redis, k_per_anchor=25, max_anchors=5)

#         logger.info("\n‚úÖ ONLINE PIPELINE READY!")
#         logger.info("="*70 + "\n")

#     # ------------------------ Public APIs -----------------------

#     def generate_feed(
#         self,
#         user_id: int,
#         limit: int = 50,
#         exclude_seen: Optional[List[int]] = None
#     ) -> List[Dict]:
#         """
#         1) Recall -> 2) Feature Extraction & ML Ranking -> 3) Reranking (business rules)
#         """
#         start_time = time.time()
#         self._last_no_feed_reasons = []
#         try:
#             # 1) Recall
#             t1 = time.time()
#             candidates, recall_diag = self._recall_candidates(user_id, target_count=self.recall_config.get("target_count", 1000))
#             self._last_recall_diag = recall_diag
#             self.metrics['recall_latency'].append((time.time() - t1) * 1000)

#             if not candidates:
#                 logger.warning(f"No candidates for user {user_id}")
#                 return []

#             # exclude seen
#             if exclude_seen:
#                 seen = set(exclude_seen)
#                 candidates = [p for p in candidates if p not in seen]

#             # 2) Ranking
#             t2 = time.time()
#             ranked_df = self.ranker.rank(user_id, candidates)
#             self.metrics['ranking_latency'].append((time.time() - t2) * 1000)
#             if ranked_df.empty:
#                 self._last_no_feed_reasons.append("ranking_empty")
#                 return []

#             # Top-K (for rerank)
#             top_k = int(self.ranking_config.get("top_k", 100))
#             ranked_df = ranked_df.head(top_k)

#             # 3) Rerank (with business rules) ‚Äî NEED post_metadata
#             t3 = time.time()
#             post_meta = self._build_post_metadata(ranked_df['post_id'].tolist())
#             final_feed = self.reranker.rerank(
#                 ranked_df=ranked_df,
#                 post_metadata=post_meta,
#                 limit=limit
#             )
#             self.metrics['reranking_latency'].append((time.time() - t3) * 1000)

#             self.metrics['total_latency'].append((time.time() - start_time) * 1000)
#             logger.info(
#                 "Feed generated for user %s: %s posts | Latency: %.1fms",
#                 user_id, len(final_feed), self.metrics['total_latency'][-1]
#             )
#             return final_feed

#         except Exception as e:
#             logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
#             return []

#     def recommend_friends(self, user_id: int, k: int = 20) -> List[Dict]:
#         # n·∫øu b·∫°n c√≥ FriendRecommendation module, g·ªçi ·ªü ƒë√¢y
#         try:
#             from recommender.online.friend_recommendation import FriendRecommendation
#             fr = FriendRecommendation(
#                 data=self.data,
#                 embeddings=self.embeddings,
#                 cf_model=self.cf_model,
#                 config=self.config.get('friend_recommendation', {})
#             )
#             return fr.recommend_friends(user_id=user_id, k=k)
#         except Exception as e:
#             logger.error(f"Error recommending friends: {e}")
#             return []

#     def update_user_embedding_realtime(self, user_id: int, post_id: int, action: str):
#         """Incremental update of user embedding after interaction"""
#         conf = self.config.get('user_embedding', {}).get('real_time_update', {})
#         if not conf.get('enabled', False):
#             return
#         triggers = conf.get('trigger_actions', [])
#         if action not in triggers:
#             return
#         try:
#             if post_id not in self.embeddings['post']:
#                 return
#             post_emb = self.embeddings['post'][post_id]
#             old_emb = self.embeddings['user'][user_id] if user_id in self.embeddings['user'] else post_emb
#             alpha = conf.get('incremental', {}).get('learning_rate', 0.1)
#             weights = {'like': 1.0, 'comment': 1.5, 'share': 2.0, 'save': 1.2, 'view': 0.5}
#             a = alpha * weights.get(action, 1.0)
#             new_emb = (1 - a) * old_emb + a * post_emb
#             new_emb = new_emb / (np.linalg.norm(new_emb) + 1e-8)
#             self.embeddings['user'][user_id] = new_emb.astype(np.float32)
#             if self.redis is not None:
#                 try:
#                     self.redis.setex(f"user:{user_id}:embedding", 604800, new_emb.tobytes())
#                 except Exception:
#                     pass
#         except Exception as e:
#             logger.error(f"update_user_embedding_realtime error: {e}")

#     def get_metrics(self) -> Dict:
#         if not self.metrics['total_latency']:
#             return {
#                 'total_requests': 0,
#                 'avg_latency_ms': 0,
#                 'p50_latency_ms': 0,
#                 'p95_latency_ms': 0,
#                 'p99_latency_ms': 0,
#                 'avg_recall_latency_ms': 0,
#                 'avg_ranking_latency_ms': 0,
#                 'avg_reranking_latency_ms': 0
#             }
#         lat = self.metrics['total_latency']
#         return {
#             'total_requests': len(lat),
#             'avg_latency_ms': float(np.mean(lat)),
#             'p50_latency_ms': float(np.percentile(lat, 50)),
#             'p95_latency_ms': float(np.percentile(lat, 95)),
#             'p99_latency_ms': float(np.percentile(lat, 99)),
#             'avg_recall_latency_ms': float(np.mean(self.metrics['recall_latency'])),
#             'avg_ranking_latency_ms': float(np.mean(self.metrics['ranking_latency'])),
#             'avg_reranking_latency_ms': float(np.mean(self.metrics['reranking_latency'])),
#         }

#     # ------------------------ Internals ------------------------

#     # def _recall_candidates(self, user_id: int, target_count: int = 1000) -> Tuple[List[int], RecallDiag]:
#     #     all_candidates: List[int] = []
#     #     diag = RecallDiag(notes={})

#     #     channels_cfg = self.recall_config.get('channels', {})
        
#     #     # Following
#     #     try:
#     #         cnt = self.recall_config['channels']['following'].get('count', 400)
#     #         following_posts = self.following_recall.recall(user_id, k=cnt)
#     #     except Exception as e:
#     #         logger.debug(f"FollowingRecall failed: {e}")
#     #         following_posts = []
#     #     diag.following = len(following_posts); all_candidates.extend(following_posts)

#     #     # CF
#     #     try:
#     #         cnt = self.recall_config['channels']['collaborative_filtering'].get('count', 300)
#     #         cf_posts = self.cf_recall.recall(user_id, k=cnt)
#     #     except Exception as e:
#     #         logger.debug(f"CFRecall failed: {e}")
#     #         cf_posts = []
#     #     diag.cf = len(cf_posts); all_candidates.extend(cf_posts)

#     #     # Content
#     #     try:
#     #         cnt = self.recall_config['channels']['content_based'].get('count', 200)
#     #         content_posts = self.content_recall.recall(user_id, k=cnt)
#     #     except Exception as e:
#     #         logger.debug(f"ContentRecall failed: {e}")
#     #         content_posts = []
#     #     diag.content = len(content_posts); all_candidates.extend(content_posts)

#     #     # Trending
#     #     try:
#     #         cnt = self.recall_config['channels']['trending'].get('count', 100)
#     #         trending_posts = self.trending_recall.recall(k=cnt)
#     #     except Exception as e:
#     #         logger.debug(f"TrendingRecall failed: {e}")
#     #         trending_posts = []
#     #     diag.trending = len(trending_posts); all_candidates.extend(trending_posts)

#     #     # Dedup & cap
#     #     unique_candidates = list(dict.fromkeys(all_candidates))[:target_count]
#     #     diag.total = len(unique_candidates)

#     #     logger.info(
#     #         "RECALL SUMMARY | user=%s | following=%s cf=%s content=%s trending=%s | total=%s",
#     #         user_id, diag.following, diag.cf, diag.content, diag.trending, diag.total
#     #     )

#     #     # basic notes
#     #     flw_cnt = self._user_following_count(user_id)
#     #     if flw_cnt == 0 and diag.following == 0:
#     #         diag.notes["following"] = "User has no following"
#     #     tw = self.recall_config['channels']['trending'].get('trending_window_hours', 6)
#     #     if diag.trending == 0 and self._trending_empty_last_hours(tw):
#     #         diag.notes["trending"] = "No posts within trending window"
#     #     if diag.content == 0 and not self._has_user_embedding(user_id):
#     #         diag.notes["content"] = "User embedding missing (cold-start)"

#     #     return unique_candidates, diag

#     def _recall_candidates(self, user_id: int, target_count: int = 1000) -> list[int]:
#         # 1. Kh·ªüi t·∫°o diag v√† danh s√°ch ·ª©ng vi√™n
#         all_candidates: List[int] = []
#         diag = RecallDiag(notes={}) 

#         channels_cfg = self.recall_config.get('channels', {})
        
#         # --- Following ---
#         try:
#             cnt = channels_cfg.get('following', {}).get('count', 400)
#             following_posts = self.following_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"FollowingRecall failed for user {user_id}: {e}")
#             following_posts = []
#         diag.following = len(following_posts)
#         all_candidates.extend(following_posts)

#         # --- CF ---
#         try:
#             cnt = channels_cfg.get('collaborative_filtering', {}).get('count', 300)
#             cf_posts = self.cf_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"CFRecall failed for user {user_id}: {e}")
#             cf_posts = []
#         diag.cf = len(cf_posts)
#         all_candidates.extend(cf_posts)

#         # --- Content ---
#         try:
#             cnt = channels_cfg.get('content_based', {}).get('count', 200)
#             content_posts = self.content_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"ContentRecall failed for user {user_id}: {e}")
#             content_posts = []
#         diag.content = len(content_posts)
#         all_candidates.extend(content_posts)

#         # --- Covisit (M·ªöI) ---
#         try:
#             # L·∫•y count t·ª´ config, m·∫∑c ƒë·ªãnh l√† 150
#             cnt = channels_cfg.get('covisit', {}).get('count', 150)
#             covisit_posts = self.covisit_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"CovisitRecall failed for user {user_id}: {e}")
#             covisit_posts = []
#         # Th√™m tr∆∞·ªùng covisit v√†o diag
#         diag.covisit = len(covisit_posts) 
#         all_candidates.extend(covisit_posts)

#         # --- Trending ---
#         try:
#             cnt = channels_cfg.get('trending', {}).get('count', 100)
#             # Trending recall th∆∞·ªùng kh√¥ng c·∫ßn user_id
#             trending_posts = self.trending_recall.recall(k=cnt) 
#         except Exception as e:
#             logger.debug(f"TrendingRecall failed: {e}")
#             trending_posts = []
#         diag.trending = len(trending_posts)
#         all_candidates.extend(trending_posts)

#         # Dedup & cap
#         # ƒê·∫£m b·∫£o gi·ªØ th·ª© t·ª± ∆∞u ti√™n c·ªßa c√°c k√™nh recall b·∫±ng dict.fromkeys
#         unique_candidates = list(dict.fromkeys(all_candidates))[:target_count]
#         diag.total = len(unique_candidates)
        
#         # 2. Ghi logs T·ªîNG K·∫æT
#         logger.info(
#             "RECALL SUMMARY | user=%s | following=%s cf=%s content=%s covisit=%s trending=%s | total=%s",
#             user_id, diag.following, diag.cf, diag.content, diag.covisit, diag.trending, diag.total
#         )

#         # basic notes
#         flw_cnt = self._user_following_count(user_id)
#         if flw_cnt == 0 and diag.following == 0:
#             diag.notes["following"] = "User has no following"
#         tw = self.recall_config['channels']['trending'].get('trending_window_hours', 6)
#         if diag.trending == 0 and self._trending_empty_last_hours(tw):
#             diag.notes["trending"] = "No posts within trending window"
#         if diag.content == 0 and not self._has_user_embedding(user_id):
#             diag.notes["content"] = "User embedding missing (cold-start)"

#         return unique_candidates, diag

#     def _safe_text(self, v) -> str:
#         """Return '' if v is NaN/None/non-string; else normalized text."""
#         if v is None:
#             return ""
#         # pandas NA/NaN
#         try:
#             import pandas as pd
#             if pd.isna(v):
#                 return ""
#         except Exception:
#             pass
#         # only accept real strings
#         if not isinstance(v, str):
#             return ""
#         return v.strip()

#     def _build_post_metadata(self, post_ids: List[int]) -> Dict[int, Dict]:
#         """
#         Return per-post metadata for reranker:
#           { post_id: {author_id, status, created_at, title, content, content_hash} }
#         """
#         meta: Dict[int, Dict] = {}

#         # 1) from loaded data (CSV/dev)
#         try:
#             post_df: pd.DataFrame = self.data["post"]
#             sub = post_df[post_df["Id"].isin(post_ids)][["Id", "UserId", "Status", "CreateDate", "Content"]].copy()
#             for _, r in sub.iterrows():
#                 pid = int(r["Id"])
#                 meta[pid] = {
#                     "author_id": int(r.get("UserId")) if not pd.isna(r.get("UserId")) else None,
#                     "status": int(r.get("Status")) if not pd.isna(r.get("Status")) else None,
#                     "created_at": pd.to_datetime(r.get("CreateDate"), errors="coerce", utc=True),
#                     "title": "",
#                     "content": (r.get("Content") or ""),
#                     "content_hash": None
#                 }
#         except Exception:
#             pass

#         # 2) Fallback: query backend DB if missing
#         missing = [pid for pid in post_ids if pid not in meta]
#         if missing and self.db_session_factory:
#             with self.db_session_factory() as s:
#                 rows = s.execute(text("""
#                     SELECT Id, UserId, Status, CreateDate, Content
#                     FROM Post WHERE Id IN :ids
#                 """), {"ids": tuple(missing)}).all()
#                 for row in rows:
#                     pid, uid, st, cd, ct = row
#                     meta[int(pid)] = {
#                         "author_id": int(uid) if uid is not None else None,
#                         "status": int(st) if st is not None else None,
#                         "created_at": pd.to_datetime(cd, errors="coerce", utc=True),
#                         "title": self._safe_text(None),
#                         "content": ct or "",
#                         "content_hash": None
#                     }

#         # 3) Hash for dedup
#         import hashlib
#         for pid, m in meta.items():
#             if not m.get("content_hash"):
#                 title = self._safe_text(m.get("title")).lower()
#                 body  = self._safe_text(m.get("content")).lower()
#                 if title or body:
#                     m["content_hash"] = hashlib.sha1(f"{title}|{body}".encode("utf-8")).hexdigest()

#         return meta

#     # ----- helpers for diag -----

#     def _user_following_count(self, user_id: int) -> int:
#         try:
#             if self.following_dict is None:
#                 return 0
#             flw = self.following_dict.get(user_id, [])
#             return len(flw) if flw is not None else 0
#         except Exception:
#             return 0

#     def _has_user_embedding(self, user_id: int) -> bool:
#         try:
#             return user_id in (self.embeddings.get("user", {}) or {})
#         except Exception:
#             return False

#     def _trending_empty_last_hours(self, hours: int) -> bool:
#         try:
#             post_df = self.data.get("post")
#             if post_df is None or post_df.empty or "CreateDate" not in post_df.columns:
#                 return True
#             dt = pd.to_datetime(post_df["CreateDate"], errors="coerce", utc=True)
#             cutoff = pd.Timestamp.utcnow() - pd.Timedelta(hours=hours)
#             return bool((dt >= cutoff).sum() == 0)
#         except Exception:
#             return True

#     # ----- backend DB utilities -----

#     def ping_backend_db(self) -> bool:
#         if self.db_engine is None:
#             return False
#         try:
#             with self.db_engine.connect() as conn:
#                 conn.execute(text("SELECT 1"))
#             return True
#         except Exception:
#             return False

#     def backend_db_stats(self) -> dict:
#         if not self.db_session_factory:
#             return {"connected": False}
#         stats = {"connected": self.ping_backend_db()}
#         try:
#             with self.db_session_factory() as s:
#                 posts = s.execute(text("SELECT COUNT(1) FROM Post")).scalar_one()
#                 users = s.execute(text("SELECT COUNT(1) FROM User")).scalar_one()
#                 reactions = s.execute(text("SELECT COUNT(1) FROM PostReaction")).scalar_one()
#                 comments = s.execute(text("SELECT COUNT(1) FROM Comment")).scalar_one()
#                 last_post = s.execute(text("SELECT MAX(CreateDate) FROM Post")).scalar_one()
#                 stats.update({
#                     "post_count": int(posts or 0),
#                     "user_count": int(users or 0),
#                     "reaction_count": int(reactions or 0),
#                     "comment_count": int(comments or 0),
#                     "last_post_at": str(last_post) if last_post else None,
#                 })
#         except Exception as e:
#             stats.update({"error": str(e)})
#         return stats

#     def get_author_id(self, post_id: int) -> Optional[int]:
#         # from dev data
#         try:
#             row = self.data["post"].loc[self.data["post"]["Id"] == post_id]
#             if not row.empty:
#                 return int(row["UserId"].iloc[0])
#         except Exception:
#             pass
#         # fallback DB
#         try:
#             if self.db_session_factory:
#                 with self.db_session_factory() as s:
#                     r = s.execute(text("SELECT UserId FROM Post WHERE Id=:pid"), {"pid": post_id}).first()
#                     return int(r[0]) if r else None
#         except Exception:
#             pass
#         return None


# """
# ONLINE INFERENCE PIPELINE - PRODUCTION READY
# ============================================
# Flow: Recall ‚Üí Feature Extraction ‚Üí Ranking ‚Üí Re-ranking ‚Üí Output

# - T∆∞∆°ng th√≠ch artifact ‚Äúlatest.version‚Äù (Windows-safe, kh√¥ng d√πng symlink b·∫Øt bu·ªôc)
# - Kh√¥ng truy·ªÅn following_dict/redis_client v√†o FeatureEngineer.__init__ (repo hi·ªán t·∫°i kh√¥ng nh·∫≠n arg n√†y)
# - G√°n following (n·∫øu c√≥) sau khi kh·ªüi t·∫°o FeatureEngineer (gi·ªØ t∆∞∆°ng th√≠ch 2 ph√≠a)
# """

# from __future__ import annotations

# import os
# import sys
# import time
# import logging
# import warnings
# from pathlib import Path
# from typing import List, Dict, Optional, Tuple
# from dataclasses import dataclass
# from collections import defaultdict

# import numpy as np
# import pandas as pd
# import yaml

# # Add project root
# sys.path.append(str(Path(__file__).parent.parent.parent))

# warnings.filterwarnings("ignore")
# logger = logging.getLogger(__name__)
# logging.getLogger("lightgbm").setLevel(logging.WARNING)

# # -------------------------------------------------------------------
# # Core & Artifacts
# # -------------------------------------------------------------------
# from recommender.offline.artifact_manager import ArtifactManager, get_latest_version_dir
# from recommender.common.feature_engineer import FeatureEngineer

# # -------------------------------------------------------------------
# # Recall channels
# # -------------------------------------------------------------------
# from recommender.online.recall import (
#     FollowingRecall,
#     CFRecall,
#     ContentRecall,
#     TrendingRecall,
# )
# from recommender.online.recall.covisit import CovisitRecall

# # -------------------------------------------------------------------
# # Ranking
# # -------------------------------------------------------------------
# from recommender.online.ranking import MLRanker, Reranker

# # -------------------------------------------------------------------
# # Redis (optional)
# # -------------------------------------------------------------------
# try:
#     import redis as _redis
#     REDIS_AVAILABLE = True
# except Exception:
#     _redis = None
#     REDIS_AVAILABLE = False
#     logger.warning("redis not available")

# # -------------------------------------------------------------------
# # Backend DB (optional)
# # -------------------------------------------------------------------
# from sqlalchemy import create_engine, text
# from sqlalchemy.orm import sessionmaker

# # Realtime helpers
# from recommender.online.realtime_handlers import RealtimeHandlers
# from recommender.online.realtime_jobs import RealTimeJobs


# @dataclass
# class RecallDiag:
#     following: int = 0
#     cf: int = 0
#     content: int = 0
#     covisit: int = 0
#     trending: int = 0
#     total: int = 0
#     notes: Dict[str, str] = None


# # =========================
# # Helper: load ranker artifacts
# # =========================
# def _load_ranker_artifacts(base_dir: str):
#     """
#     Load ranker artifacts theo format:
#       models/<version>/
#         - ranker_model.txt
#         - ranker_scaler.pkl
#         - ranker_feature_cols.pkl
#         - meta.json (optional)
#     ∆Øu ti√™n ƒë·ªçc latest.version / symlink latest.
#     Tr·∫£ v·ªÅ: (model, scaler, feature_cols, meta_dict, version_dir)
#     """
#     import json
#     import pickle
#     from lightgbm import Booster

#     base = Path(base_dir)

#     # 1) Resolve version dir b·∫±ng offline.artifact_manager API
#     vdir = get_latest_version_dir(str(base))
#     if vdir is None:
#         # fallback: n·∫øu config ch·ªâ ƒë·ªãnh th·∫≥ng folder
#         latest_file = base / "latest.version"
#         if latest_file.exists():
#             ver = latest_file.read_text(encoding="utf-8").strip()
#             vdir = base / ver
#         elif (base / "latest").is_symlink():
#             try:
#                 vdir = (base / "latest").resolve(strict=True)
#             except Exception:
#                 vdir = None
#     if vdir is None:
#         raise FileNotFoundError(
#             f"Cannot resolve latest model version under {base_dir}. "
#             f"Missing latest.version or latest symlink."
#         )

#     # 2) File names
#     f_model = vdir / "ranker_model.txt"
#     f_scaler = vdir / "ranker_scaler.pkl"
#     f_cols  = vdir / "ranker_feature_cols.pkl"
#     f_meta  = vdir / "meta.json"

#     # 3) Load
#     if not f_model.exists():
#         raise FileNotFoundError(f"Missing model file: {f_model}")
#     model = Booster(model_file=str(f_model))

#     scaler = None
#     if f_scaler.exists():
#         with f_scaler.open("rb") as f:
#             scaler = pickle.load(f)

#     feature_cols = None
#     if f_cols.exists():
#         with f_cols.open("rb") as f:
#             feature_cols = pickle.load(f)

#     meta = {}
#     if f_meta.exists():
#         try:
#             meta = json.loads(f_meta.read_text(encoding="utf-8"))
#         except Exception:
#             meta = {}

#     return model, scaler, feature_cols, meta, vdir


# class OnlineInferencePipeline:
#     """
#     Complete online inference pipeline

#     Features:
#     - Multi-channel recall (following / CF / content / covisit / trending)
#     - ML ranking with LightGBM (t∆∞∆°ng th√≠ch artifact m·ªõi & c≈©)
#     - Business rules re-ranking
#     - Real-time user embedding updates
#     - Redis caching (∆∞u ti√™n), fallback MySQL
#     """

#     def __init__(
#         self,
#         config_path: str = "configs/config_online.yaml",
#         models_dir: str = "models",
#         data_dir: str = "dataset",
#         use_redis: bool = True,
#     ):
#         self.models_dir = Path(models_dir)
#         self.data_dir = Path(data_dir)

#         logger.info("\n" + "=" * 70)
#         logger.info("INITIALIZING ONLINE INFERENCE PIPELINE")
#         logger.info("=" * 70)

#         # ----------------- Step 1: CONFIG ----------------------
#         with open(config_path, "r", encoding="utf-8") as f:
#             self.config = yaml.safe_load(f) or {}

#         self.recall_config = self.config.get("recall", {}) or {}
#         self.ranking_config = self.config.get("ranking", {}) or {}
#         self.reranking_config = self.config.get("reranking", {}) or {}

#         # ----------------- Step 2: REDIS -----------------------
#         self.redis: Optional[_redis.Redis] = None
#         if use_redis and REDIS_AVAILABLE:
#             try:
#                 rc = self.config.get("redis", {}) or {}
#                 url = rc.get("url")
#                 if url:
#                     self.redis = _redis.from_url(url, decode_responses=True)
#                 else:
#                     self.redis = _redis.Redis(
#                         host=rc.get("host", "localhost"),
#                         port=rc.get("port", 6379),
#                         db=rc.get("db", 0),
#                         decode_responses=True,
#                         socket_timeout=rc.get("socket_timeout", 5),
#                         max_connections=rc.get("max_connections", 50),
#                     )
#                 self.redis.ping()
#                 logger.info("‚úÖ Redis connected")
#             except Exception as e:
#                 logger.warning(f"Redis connection failed: {e}")
#                 self.redis = None
#         else:
#             logger.warning("Redis disabled or not available")

#         # ----------------- Step 2b: BACKEND DB (optional) ------
#         self.db_engine = None
#         self.db_session_factory = None
#         try:
#             db = self.config.get("database", {}) or self.config.get("backend_db", {}) or {}
#             db_url = db.get("url")
#             if db_url:
#                 self.db_engine = create_engine(
#                     db_url,
#                     pool_size=db.get("pool_size", 20),
#                     max_overflow=db.get("max_overflow", 40),
#                     pool_recycle=db.get("pool_recycle", 1800),
#                     pool_pre_ping=db.get("pool_pre_ping", True),
#                     future=True,
#                 )
#                 self.db_session_factory = sessionmaker(
#                     bind=self.db_engine, autocommit=False, autoflush=False, future=True
#                 )
#                 with self.db_engine.connect() as conn:
#                     conn.execute(text("SELECT 1"))
#                 logger.info("‚úÖ Backend DB connected")
#             else:
#                 logger.info("Backend DB not configured")
#         except Exception as e:
#             logger.warning(f"Backend DB connection failed: {e}")
#             self.db_engine = None
#             self.db_session_factory = None

#         # ----------------- Step 3: ARTIFACTS -------------------
#         logger.info("\nüì¶ Loading model artifacts...")

#         # ArtifactManager: ch·ªâ d√πng ƒë·ªÉ resolve version dir 'latest'
#         self.artifact_mgr = ArtifactManager(artifacts_base_dir=str(self.models_dir))

#         req_version = self.config.get("models", {}).get("version", "latest")
#         if req_version == "latest":
#             vdir = self.artifact_mgr.get_latest_version_dir()
#             if not vdir:
#                 raise RuntimeError("No model version found in models/ (missing latest.version or symlink latest)")
#             self.current_version = vdir.name
#         else:
#             self.current_version = req_version

#         # M·∫∑c ƒë·ªãnh c√°c artifact kh√¥ng b·∫Øt bu·ªôc (embeddings/cf‚Ä¶) ‚Äî ƒë·ªÉ compatible khi ch∆∞a c√≥
#         self.embeddings = {}
#         self.faiss_index = None
#         self.faiss_post_ids = []
#         self.cf_model = None
#         self.user_stats = {}
#         self.author_stats = {}
#         self.following_dict = {}
#         self.metadata = {}

#         # Load ranker_* theo format m·ªõi
#         (
#             self.ranking_model,
#             self.ranking_scaler,
#             self.ranking_feature_cols,
#             meta2,
#             ver_dir,
#         ) = _load_ranker_artifacts(base_dir=str(self.models_dir))
#         self.metadata.update(meta2 or {})
#         self._version_dir = ver_dir

#         logger.info("‚úÖ Ranker artifacts loaded")

#         # ----------------- Step 4: DATA (dev/feature) ----------
#         logger.info("\nüìä Loading data...")
#         # L∆∞u √Ω: feature_engineer.py hi·ªán k·ª≥ v·ªçng keys: users, posts, friendships, post_hashtags
#         # H√†m load_data (n·∫øu c·ªßa b·∫°n tr·∫£ v·ªÅ user/post/...) th√¨ ta normalize l·∫°i.
#         from recommender.common.data_loader import load_data
#         raw = load_data(str(self.data_dir)) or {}
#         self.data = self._normalize_data_keys(raw)
#         logger.info("‚úÖ Data loaded")

#         # ----------------- Step 5: FEATURE ENGINEER ------------
#         # ‚ö†Ô∏è FeatureEngineer trong repo hi·ªán t·∫°i KH√îNG nh·∫≠n following_dict/redis_client
#         self.feature_engineer = FeatureEngineer(
#             data=self.data,
#             user_stats=self.user_stats,
#             author_stats=self.author_stats,
#             following=self.following_dict,          # mapping ƒë√∫ng v·ªõi dataclass
#             embeddings=self.embeddings,
#         )
#         # n·∫øu class c√≥ set_following_dict() / thu·ªôc t√≠nh following: v·∫´n g√°n cho ch·∫Øc
#         try:
#             if hasattr(self.feature_engineer, "set_following_dict"):
#                 self.feature_engineer.set_following_dict(self.following_dict or {})
#             elif hasattr(self.feature_engineer, "following"):
#                 setattr(self.feature_engineer, "following", self.following_dict or {})
#         except Exception:
#             pass

#         # ----------------- Step 6: RECALL CHANNELS -------------
#         channels_config = self.recall_config.get("channels", {}) or {}
#         self.following_recall = FollowingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             following_dict=self.following_dict,
#             config=channels_config.get("following", {}) or {},
#         )
#         self.cf_recall = CFRecall(
#             redis_client=self.redis,
#             data=self.data,
#             cf_model=self.cf_model,
#             config=channels_config.get("collaborative_filtering", {}) or {},
#         )
#         self.content_recall = ContentRecall(
#             redis_client=self.redis,
#             embeddings=self.embeddings,
#             faiss_index=self.faiss_index,
#             faiss_post_ids=self.faiss_post_ids,
#             data=self.data,
#             config=channels_config.get("content_based", {}) or {},
#         )
#         self.trending_recall = TrendingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             config=channels_config.get("trending", {}) or {},
#         )
#         self.covisit_recall = CovisitRecall(self.redis, k_per_anchor=25, max_anchors=5)

#         # ----------------- Step 7: RANKER & RERANK -------------
#         self.ranker = MLRanker(
#             model=self.ranking_model,
#             scaler=self.ranking_scaler,
#             feature_cols=self.ranking_feature_cols,
#             feature_engineer=self.feature_engineer,
#             config=self.ranking_config,
#         )
#         self.reranker = Reranker(config=self.reranking_config)

#         # ----------------- METRICS -----------------------------
#         self.metrics = defaultdict(list)
#         self._last_recall_diag: Optional[RecallDiag] = None
#         self._last_no_feed_reasons: List[str] = []

#         # Realtime helpers
#         self.rt_handlers = RealtimeHandlers(
#             self.redis,
#             action_weights=self.config.get("user_embedding", {})
#             .get("real_time_update", {})
#             .get("action_weights", None),
#         )
#         self.rt_jobs = RealTimeJobs(
#             self.redis,
#             interval_trending=self.recall_config.get("channels", {})
#             .get("trending", {})
#             .get("refresh_interval_seconds", 300),
#             interval_post_feat=900,
#         )
#         self.rt_jobs.start()

#         logger.info("\n‚úÖ ONLINE PIPELINE READY!")
#         logger.info("=" * 70 + "\n")

#     # ------------------------------------------------------------------
#     # PUBLIC APIS
#     # ------------------------------------------------------------------
#     def generate_feed(
#         self, user_id: int, limit: int = 50, exclude_seen: Optional[List[int]] = None
#     ) -> List[Dict]:
#         """
#         1) Recall -> 2) Feature Extraction & ML Ranking -> 3) Reranking
#         """
#         start_time = time.time()
#         self._last_no_feed_reasons = []
#         try:
#             # 1) Recall
#             t1 = time.time()
#             candidates, recall_diag = self._recall_candidates(
#                 user_id, target_count=self.recall_config.get("target_count", 1000)
#             )
#             self._last_recall_diag = recall_diag
#             self.metrics["recall_latency"].append((time.time() - t1) * 1000)

#             if not candidates:
#                 logger.warning(f"No candidates for user {user_id}")
#                 return []

#             # exclude seen
#             if exclude_seen:
#                 seen = set(exclude_seen)
#                 candidates = [p for p in candidates if p not in seen]

#             # 2) Ranking
#             t2 = time.time()
#             ranked_df = self.ranker.rank(user_id, candidates)
#             self.metrics["ranking_latency"].append((time.time() - t2) * 1000)
#             if ranked_df.empty:
#                 self._last_no_feed_reasons.append("ranking_empty")
#                 return []

#             # Top-K (for rerank)
#             top_k = int(self.ranking_config.get("top_k", 100))
#             ranked_df = ranked_df.head(top_k)

#             # 3) Reranking v·ªõi business rules
#             t3 = time.time()
#             post_meta = self._build_post_metadata(ranked_df["post_id"].tolist())
#             final_feed = self.reranker.rerank(
#                 ranked_df=ranked_df, post_metadata=post_meta, limit=limit
#             )
#             self.metrics["reranking_latency"].append((time.time() - t3) * 1000)

#             self.metrics["total_latency"].append((time.time() - start_time) * 1000)
#             logger.info(
#                 "Feed generated for user %s: %s posts | Latency: %.1fms",
#                 user_id,
#                 len(final_feed),
#                 self.metrics["total_latency"][-1],
#             )
#             return final_feed
#         except Exception as e:
#             logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
#             return []

#     def recommend_friends(self, user_id: int, k: int = 20) -> List[Dict]:
#         try:
#             from recommender.online.friend_recommendation import FriendRecommendation
#             fr = FriendRecommendation(
#                 data=self.data,
#                 embeddings=self.embeddings,
#                 cf_model=self.cf_model,
#                 config=self.config.get("friend_recommendation", {}) or {},
#             )
#             return fr.recommend_friends(user_id=user_id, k=k)
#         except Exception as e:
#             logger.error(f"Error recommending friends: {e}")
#             return []

#     def update_user_embedding_realtime(self, user_id: int, post_id: int, action: str):
#         """Incremental update of user embedding after interaction"""
#         conf = self.config.get("user_embedding", {}).get("real_time_update", {}) or {}
#         if not conf.get("enabled", False):
#             return
#         triggers = conf.get("trigger_actions", []) or []
#         if action not in triggers:
#             return
#         try:
#             if post_id not in (self.embeddings.get("post", {}) or {}):
#                 return
#             post_emb = self.embeddings["post"][post_id]
#             old_emb = (
#                 self.embeddings["user"][user_id]
#                 if user_id in (self.embeddings.get("user", {}) or {})
#                 else post_emb
#             )
#             alpha = conf.get("incremental", {}).get("learning_rate", 0.1)
#             weights = {"like": 1.0, "comment": 1.5, "share": 2.0, "save": 1.2, "view": 0.5}
#             a = alpha * weights.get(action, 1.0)
#             new_emb = (1 - a) * old_emb + a * post_emb
#             new_emb = new_emb / (np.linalg.norm(new_emb) + 1e-8)
#             self.embeddings.setdefault("user", {})[user_id] = new_emb.astype(np.float32)
#             if self.redis is not None:
#                 try:
#                     self.redis.setex(
#                         f"user:{user_id}:embedding", 7 * 24 * 3600, new_emb.tobytes()
#                     )
#                 except Exception:
#                     pass
#         except Exception as e:
#             logger.error(f"update_user_embedding_realtime error: {e}")

#     def get_metrics(self) -> Dict:
#         if not self.metrics["total_latency"]:
#             return {
#                 "total_requests": 0,
#                 "avg_latency_ms": 0,
#                 "p50_latency_ms": 0,
#                 "p95_latency_ms": 0,
#                 "p99_latency_ms": 0,
#                 "avg_recall_latency_ms": 0,
#                 "avg_ranking_latency_ms": 0,
#                 "avg_reranking_latency_ms": 0,
#             }
#         lat = self.metrics["total_latency"]
#         return {
#             "total_requests": len(lat),
#             "avg_latency_ms": float(np.mean(lat)),
#             "p50_latency_ms": float(np.percentile(lat, 50)),
#             "p95_latency_ms": float(np.percentile(lat, 95)),
#             "p99_latency_ms": float(np.percentile(lat, 99)),
#             "avg_recall_latency_ms": float(np.mean(self.metrics["recall_latency"])),
#             "avg_ranking_latency_ms": float(np.mean(self.metrics["ranking_latency"])),
#             "avg_reranking_latency_ms": float(np.mean(self.metrics["reranking_latency"])),
#         }

#     # ------------------------------------------------------------------
#     # INTERNALS
#     # ------------------------------------------------------------------
#     def _normalize_data_keys(self, raw: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
#         """
#         Chu·∫©n ho√° keys ƒë·ªÉ kh·ªõp FeatureEngineer hi·ªán t·∫°i:
#           FE k·ª≥ v·ªçng: users, posts, friendships, post_hashtags
#           (n·∫øu loader tr·∫£: user, post, friendship, post_hashtag th√¨ map l·∫°i)
#         """
#         if raw is None:
#             return {}
#         data = dict(raw)
#         # map ph·ªï bi·∫øn
#         if "users" not in data and "user" in data: data["users"] = data["user"]
#         if "posts" not in data and "post" in data: data["posts"] = data["post"]
#         if "friendships" not in data and "friendship" in data: data["friendships"] = data["friendship"]
#         if "post_hashtags" not in data and "post_hashtag" in data: data["post_hashtags"] = data["post_hashtag"]
#         return data

#     def _recall_candidates(
#         self, user_id: int, target_count: int = 1000
#     ) -> Tuple[List[int], RecallDiag]:
#         all_candidates: List[int] = []
#         diag = RecallDiag(notes={})

#         channels_cfg = self.recall_config.get("channels", {}) or {}

#         # Following
#         try:
#             cnt = channels_cfg.get("following", {}).get("count", 400)
#             following_posts = self.following_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"FollowingRecall failed for user {user_id}: {e}")
#             following_posts = []
#         diag.following = len(following_posts)
#         all_candidates.extend(following_posts)

#         # CF
#         try:
#             cnt = channels_cfg.get("collaborative_filtering", {}).get("count", 300)
#             cf_posts = self.cf_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"CFRecall failed for user {user_id}: {e}")
#             cf_posts = []
#         diag.cf = len(cf_posts)
#         all_candidates.extend(cf_posts)

#         # Content
#         try:
#             cnt = channels_cfg.get("content_based", {}).get("count", 200)
#             content_posts = self.content_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"ContentRecall failed for user {user_id}: {e}")
#             content_posts = []
#         diag.content = len(content_posts)
#         all_candidates.extend(content_posts)

#         # Covisit
#         try:
#             cnt = channels_cfg.get("covisit", {}).get("count", 150)
#             covisit_posts = self.covisit_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"CovisitRecall failed for user {user_id}: {e}")
#             covisit_posts = []
#         diag.covisit = len(covisit_posts)
#         all_candidates.extend(covisit_posts)

#         # Trending
#         try:
#             cnt = channels_cfg.get("trending", {}).get("count", 100)
#             trending_posts = self.trending_recall.recall(k=cnt)
#         except Exception as e:
#             logger.debug(f"TrendingRecall failed: {e}")
#             trending_posts = []
#         diag.trending = len(trending_posts)
#         all_candidates.extend(trending_posts)

#         unique_candidates = list(dict.fromkeys(all_candidates))[:target_count]
#         diag.total = len(unique_candidates)

#         logger.info(
#             "RECALL SUMMARY | user=%s | following=%s cf=%s content=%s covisit=%s trending=%s | total=%s",
#             user_id,
#             diag.following,
#             diag.cf,
#             diag.content,
#             diag.covisit,
#             diag.trending,
#             diag.total,
#         )

#         # Notes
#         flw_cnt = self._user_following_count(user_id)
#         if flw_cnt == 0 and diag.following == 0:
#             diag.notes["following"] = "User has no following"
#         tw = channels_cfg.get("trending", {}).get("trending_window_hours", 6)
#         if diag.trending == 0 and self._trending_empty_last_hours(tw):
#             diag.notes["trending"] = "No posts within trending window"
#         if diag.content == 0 and not self._has_user_embedding(user_id):
#             diag.notes["content"] = "User embedding missing (cold-start)"

#         return unique_candidates, diag

#     def _safe_text(self, v) -> str:
#         if v is None:
#             return ""
#         try:
#             if pd.isna(v):
#                 return ""
#         except Exception:
#             pass
#         if not isinstance(v, str):
#             return ""
#         return v.strip()

#     def _build_post_metadata(self, post_ids: List[int]) -> Dict[int, Dict]:
#         meta: Dict[int, Dict] = {}

#         # 1) t·ª´ data ƒë√£ load (CSV/dev) ‚Äî ch√∫ √Ω: theo FE keys l√† 'posts'
#         try:
#             post_df: pd.DataFrame = self.data["posts"]
#             sub = post_df[post_df["Id"].isin(post_ids)][
#                 ["Id", "UserId", "Status", "CreateDate", "Content"]
#             ].copy()
#             for _, r in sub.iterrows():
#                 pid = int(r["Id"])
#                 meta[pid] = {
#                     "author_id": int(r.get("UserId"))
#                     if not pd.isna(r.get("UserId"))
#                     else None,
#                     "status": int(r.get("Status")) if not pd.isna(r.get("Status")) else None,
#                     "created_at": pd.to_datetime(r.get("CreateDate"), errors="coerce", utc=True),
#                     "title": "",
#                     "content": (r.get("Content") or ""),
#                     "content_hash": None,
#                 }
#         except Exception:
#             pass

#         # 2) Fallback DB n·∫øu thi·∫øu
#         missing = [pid for pid in post_ids if pid not in meta]
#         if missing and self.db_session_factory:
#             with self.db_session_factory() as s:
#                 rows = s.execute(
#                     text(
#                         "SELECT Id, UserId, Status, CreateDate, Content FROM Post WHERE Id IN :ids"
#                     ),
#                     {"ids": tuple(missing)},
#                 ).all()
#                 for row in rows:
#                     pid, uid, st, cd, ct = row
#                     meta[int(pid)] = {
#                         "author_id": int(uid) if uid is not None else None,
#                         "status": int(st) if st is not None else None,
#                         "created_at": pd.to_datetime(cd, errors="coerce", utc=True),
#                         "title": self._safe_text(None),
#                         "content": ct or "",
#                         "content_hash": None,
#                     }

#         # 3) Hash n·ªôi dung cho dedup
#         import hashlib
#         for pid, m in meta.items():
#             if not m.get("content_hash"):
#                 title = self._safe_text(m.get("title")).lower()
#                 body = self._safe_text(m.get("content")).lower()
#                 if title or body:
#                     m["content_hash"] = hashlib.sha1(f"{title}|{body}".encode("utf-8")).hexdigest()

#         return meta

#     # ----- helpers for diag -----
#     def _user_following_count(self, user_id: int) -> int:
#         try:
#             if self.following_dict is None:
#                 return 0
#             flw = self.following_dict.get(user_id, [])
#             return len(flw) if flw is not None else 0
#         except Exception:
#             return 0

#     def _has_user_embedding(self, user_id: int) -> bool:
#         try:
#             return user_id in (self.embeddings.get("user", {}) or {})
#         except Exception:
#             return False

#     def _trending_empty_last_hours(self, hours: int) -> bool:
#         try:
#             post_df = self.data.get("posts")
#             if post_df is None or post_df.empty or "CreateDate" not in post_df.columns:
#                 return True
#             dt = pd.to_datetime(post_df["CreateDate"], errors="coerce", utc=True)
#             cutoff = pd.Timestamp.utcnow() - pd.Timedelta(hours=hours)
#             return bool((dt >= cutoff).sum() == 0)
#         except Exception:
#             return True

#     # ----- backend DB utilities -----
#     def ping_backend_db(self) -> bool:
#         if self.db_engine is None:
#             return False
#         try:
#             with self.db_engine.connect() as conn:
#                 conn.execute(text("SELECT 1"))
#             return True
#         except Exception:
#             return False

#     def backend_db_stats(self) -> dict:
#         if not self.db_session_factory:
#             return {"connected": False}
#         stats = {"connected": self.ping_backend_db()}
#         try:
#             with self.db_session_factory() as s:
#                 posts = s.execute(text("SELECT COUNT(1) FROM Post")).scalar_one()
#                 users = s.execute(text("SELECT COUNT(1) FROM User")).scalar_one()
#                 reactions = s.execute(text("SELECT COUNT(1) FROM PostReaction")).scalar_one()
#                 comments = s.execute(text("SELECT COUNT(1) FROM Comment")).scalar_one()
#                 last_post = s.execute(text("SELECT MAX(CreateDate) FROM Post")).scalar_one()
#                 stats.update(
#                     {
#                         "post_count": int(posts or 0),
#                         "user_count": int(users or 0),
#                         "reaction_count": int(reactions or 0),
#                         "comment_count": int(comments or 0),
#                         "last_post_at": str(last_post) if last_post else None,
#                     }
#                 )
#         except Exception as e:
#             stats.update({"error": str(e)})
#         return stats

#     def get_author_id(self, post_id: int) -> Optional[int]:
#         # t·ª´ data ƒë√£ load
#         try:
#             row = self.data["posts"].loc[self.data["posts"]["Id"] == post_id]
#             if not row.empty:
#                 return int(row["UserId"].iloc[0])
#         except Exception:
#             pass
#         # fallback DB
#         try:
#             if self.db_session_factory:
#                 with self.db_session_factory() as s:
#                     r = s.execute(
#                         text("SELECT UserId FROM Post WHERE Id=:pid"), {"pid": post_id}
#                     ).first()
#                     return int(r[0]) if r else None
#         except Exception:
#             pass
#         return None



# # recommender/online/online_inference_pipeline.py
# """
# ONLINE INFERENCE PIPELINE - PRODUCTION READY
# ============================================
# Flow: Recall ‚Üí Feature Extraction ‚Üí Ranking ‚Üí Re-ranking ‚Üí Output

# - T∆∞∆°ng th√≠ch artifact ‚Äúlatest.version‚Äù (Windows-safe)
# - Th√™m Seen-Tracking v·ªõi Redis ZSET user:{id}:seen_posts (TTL 7d)
# - G·ªôp recall logic; gi·ªØ covisit/trending; batch rank
# """

# from __future__ import annotations

# import os
# import sys
# import time
# import logging
# import warnings
# from pathlib import Path
# from typing import List, Dict, Optional, Tuple
# from dataclasses import dataclass
# from collections import defaultdict

# import numpy as np
# import pandas as pd
# import yaml

# sys.path.append(str(Path(__file__).parent.parent.parent))

# warnings.filterwarnings("ignore")
# logger = logging.getLogger(__name__)
# logging.getLogger("lightgbm").setLevel(logging.WARNING)

# from recommender.offline.artifact_manager import ArtifactManager, get_latest_version_dir
# from recommender.common.feature_engineer import FeatureEngineer

# from recommender.online.recall import (
#     FollowingRecall,
#     CFRecall,
#     ContentRecall,
#     TrendingRecall,
# )
# from recommender.online.recall.covisit import CovisitRecall

# from recommender.online.ranking import MLRanker, Reranker

# try:
#     import redis as _redis
#     REDIS_AVAILABLE = True
# except Exception:
#     _redis = None
#     REDIS_AVAILABLE = False
#     logger.warning("redis not available")

# from sqlalchemy import create_engine, text
# from sqlalchemy.orm import sessionmaker

# from recommender.online.realtime_handlers import RealtimeHandlers
# from recommender.online.realtime_jobs import RealTimeJobs

# @dataclass
# class RecallDiag:
#     following: int = 0
#     cf: int = 0
#     content: int = 0
#     covisit: int = 0
#     trending: int = 0
#     total: int = 0
#     notes: Dict[str, str] = None

# def _load_ranker_artifacts(base_dir: str):
#     import json
#     import pickle
#     from lightgbm import Booster

#     base = Path(base_dir)
#     vdir = get_latest_version_dir(str(base))
#     if vdir is None:
#         latest_file = base / "latest.version"
#         if latest_file.exists():
#             ver = latest_file.read_text(encoding="utf-8").strip()
#             vdir = base / ver
#         elif (base / "latest").is_symlink():
#             try:
#                 vdir = (base / "latest").resolve(strict=True)
#             except Exception:
#                 vdir = None
#     if vdir is None:
#         raise FileNotFoundError(
#             f"Cannot resolve latest model version under {base_dir}. Missing latest.version or latest symlink."
#         )

#     f_model = vdir / "ranker_model.txt"
#     f_scaler = vdir / "ranker_scaler.pkl"
#     f_cols  = vdir / "ranker_feature_cols.pkl"
#     f_meta  = vdir / "meta.json"

#     if not f_model.exists():
#         raise FileNotFoundError(f"Missing model file: {f_model}")
#     model = Booster(model_file=str(f_model))

#     scaler = None
#     if f_scaler.exists():
#         with f_scaler.open("rb") as f:
#             scaler = pickle.load(f)

#     feature_cols = None
#     if f_cols.exists():
#         with f_cols.open("rb") as f:
#             feature_cols = pickle.load(f)

#     meta = {}
#     if f_meta.exists():
#         try:
#             meta = json.loads(f_meta.read_text(encoding="utf-8"))
#         except Exception:
#             meta = {}

#     return model, scaler, feature_cols, meta, vdir

# class OnlineInferencePipeline:
#     def __init__(
#         self,
#         config_path: str = "configs/config_online.yaml",
#         models_dir: str = "models",
#         data_dir: str = "dataset",
#         use_redis: bool = True,
#     ):
#         self.models_dir = Path(models_dir)
#         self.data_dir = Path(data_dir)

#         logger.info("\n" + "=" * 70)
#         logger.info("INITIALIZING ONLINE INFERENCE PIPELINE")
#         logger.info("=" * 70)

#         with open(config_path, "r", encoding="utf-8") as f:
#             self.config = yaml.safe_load(f) or {}

#         self.recall_config = self.config.get("recall", {}) or {}
#         self.ranking_config = self.config.get("ranking", {}) or {}
#         self.reranking_config = self.config.get("reranking", {}) or {}

#         # Redis
#         self.redis: Optional[_redis.Redis] = None
#         if use_redis and REDIS_AVAILABLE:
#             try:
#                 rc = self.config.get("redis", {}) or {}
#                 url = rc.get("url")
#                 if url:
#                     self.redis = _redis.from_url(url, decode_responses=True)
#                 else:
#                     self.redis = _redis.Redis(
#                         host=rc.get("host", "localhost"),
#                         port=rc.get("port", 6379),
#                         db=rc.get("db", 0),
#                         decode_responses=True,
#                         socket_timeout=rc.get("socket_timeout", 5),
#                         max_connections=rc.get("max_connections", 50),
#                     )
#                 self.redis.ping()
#                 logger.info("‚úÖ Redis connected")
#             except Exception as e:
#                 logger.warning(f"Redis connection failed: {e}")
#                 self.redis = None
#         else:
#             logger.warning("Redis disabled or not available")

#         # TTL for seen
#         ttl_cfg = (self.config.get("redis", {}).get("ttl", {}) if self.config.get("redis") else {}) or {}
#         self.seen_ttl = int(ttl_cfg.get("seen_posts", 7 * 24 * 3600))

#         # Backend DB
#         self.db_engine = None
#         self.db_session_factory = None
#         try:
#             db = self.config.get("database", {}) or self.config.get("backend_db", {}) or {}
#             db_url = db.get("url")
#             if db_url:
#                 self.db_engine = create_engine(
#                     db_url,
#                     pool_size=db.get("pool_size", 20),
#                     max_overflow=db.get("max_overflow", 40),
#                     pool_recycle=db.get("pool_recycle", 1800),
#                     pool_pre_ping=db.get("pool_pre_ping", True),
#                     future=True,
#                 )
#                 self.db_session_factory = sessionmaker(
#                     bind=self.db_engine, autocommit=False, autoflush=False, future=True
#                 )
#                 with self.db_engine.connect() as conn:
#                     conn.execute(text("SELECT 1"))
#                 logger.info("‚úÖ Backend DB connected")
#             else:
#                 logger.info("Backend DB not configured")
#         except Exception as e:
#             logger.warning(f"Backend DB connection failed: {e}")
#             self.db_engine = None
#             self.db_session_factory = None

#         # Artifacts
#         logger.info("\nüì¶ Loading model artifacts...")
#         self.artifact_mgr = ArtifactManager(artifacts_base_dir=str(self.models_dir))

#         req_version = self.config.get("models", {}).get("version", "latest")
#         if req_version == "latest":
#             vdir = self.artifact_mgr.get_latest_version_dir()
#             if not vdir:
#                 raise RuntimeError("No model version found in models/ (missing latest.version or symlink latest)")
#             self.current_version = vdir.name
#         else:
#             self.current_version = req_version

#         self.embeddings = {}
#         self.faiss_index = None
#         self.faiss_post_ids = []
#         self.cf_model = None
#         self.user_stats = {}
#         self.author_stats = {}
#         self.following_dict = {}
#         self.metadata = {}

#         self.ranking_model, self.ranking_scaler, self.ranking_feature_cols, meta2, ver_dir = _load_ranker_artifacts(
#             base_dir=str(self.models_dir)
#         )
#         self.metadata.update(meta2 or {})
#         self._version_dir = ver_dir
#         logger.info("‚úÖ Ranker artifacts loaded")

#         # Data
#         logger.info("\nüìä Loading data...")
#         from recommender.common.data_loader import load_data
#         raw = load_data(str(self.data_dir)) or {}
#         self.data = self._normalize_data_keys(raw)
#         logger.info("‚úÖ Data loaded")

#         # Feature Engineer
#         self.feature_engineer = FeatureEngineer(
#             data=self.data,
#             user_stats=self.user_stats,
#             author_stats=self.author_stats,
#             following=self.following_dict,
#             embeddings=self.embeddings,
#         )
#         try:
#             if hasattr(self.feature_engineer, "set_following_dict"):
#                 self.feature_engineer.set_following_dict(self.following_dict or {})
#             elif hasattr(self.feature_engineer, "following"):
#                 setattr(self.feature_engineer, "following", self.following_dict or {})
#         except Exception:
#             pass

#         # Recall channels
#         channels_config = self.recall_config.get("channels", {}) or {}
#         self.following_recall = FollowingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             following_dict=self.following_dict,
#             config=channels_config.get("following", {}) or {},
#         )
#         self.cf_recall = CFRecall(
#             redis_client=self.redis,
#             data=self.data,
#             cf_model=self.cf_model,
#             config=channels_config.get("collaborative_filtering", {}) or {},
#         )
#         self.content_recall = ContentRecall(
#             redis_client=self.redis,
#             embeddings=self.embeddings,
#             faiss_index=self.faiss_index,
#             faiss_post_ids=self.faiss_post_ids,
#             data=self.data,
#             config=channels_config.get("content_based", {}) or {},
#         )
#         self.trending_recall = TrendingRecall(
#             redis_client=self.redis,
#             data=self.data,
#             config=channels_config.get("trending", {}) or {},
#         )
#         self.covisit_recall = CovisitRecall(self.redis, k_per_anchor=25, max_anchors=5)

#         # Ranker & Reranker
#         self.ranker = MLRanker(
#             model=self.ranking_model,
#             scaler=self.ranking_scaler,
#             feature_cols=self.ranking_feature_cols,
#             feature_engineer=self.feature_engineer,
#             config=self.ranking_config,
#         )
#         self.reranker = Reranker(config=self.reranking_config)

#         # Metrics
#         self.metrics = defaultdict(list)
#         self._last_recall_diag: Optional[RecallDiag] = None
#         self._last_no_feed_reasons: List[str] = []

#         # Realtime jobs
#         self.rt_handlers = RealtimeHandlers(
#             self.redis,
#             action_weights=self.config.get("user_embedding", {})
#             .get("real_time_update", {})
#             .get("action_weights", None),
#         )
#         self.rt_jobs = RealTimeJobs(
#             self.redis,
#             interval_trending=self.recall_config.get("channels", {})
#             .get("trending", {})
#             .get("refresh_interval_seconds", 300),
#             interval_post_feat=900,
#         )
#         self.rt_jobs.start()

#         logger.info("\n‚úÖ ONLINE PIPELINE READY!")
#         logger.info("=" * 70 + "\n")

#     # --------------------------- PUBLIC APIS ---------------------------
#     def generate_feed(
#         self, user_id: int, limit: int = 50, exclude_seen: Optional[List[int]] = None
#     ) -> List[Dict]:
#         start_time = time.time()
#         self._last_no_feed_reasons = []
#         try:
#             # Recall
#             t1 = time.time()
#             candidates, recall_diag = self._recall_candidates(
#                 user_id, target_count=self.recall_config.get("target_count", 1000)
#             )
#             self._last_recall_diag = recall_diag
#             self.metrics["recall_latency"].append((time.time() - t1) * 1000)

#             if not candidates:
#                 logger.warning(f"No candidates for user {user_id}")
#                 return []

#             # Auto-exclude seen via Redis ZSET
#             candidates = self._filter_seen(user_id, candidates)

#             # Extra exclude list (API param)
#             if exclude_seen:
#                 seen = set(exclude_seen)
#                 candidates = [p for p in candidates if p not in seen]

#             if not candidates:
#                 self._last_no_feed_reasons.append("all_seen")
#                 return []

#             # Ranking
#             t2 = time.time()
#             ranked_df = self.ranker.rank(user_id, candidates)
#             self.metrics["ranking_latency"].append((time.time() - t2) * 1000)
#             if ranked_df.empty:
#                 self._last_no_feed_reasons.append("ranking_empty")
#                 return []

#             # Top-K
#             top_k = int(self.ranking_config.get("top_k", 100))
#             ranked_df = ranked_df.head(top_k)

#             # Reranking
#             t3 = time.time()
#             post_meta = self._build_post_metadata(ranked_df["post_id"].tolist())
#             final_feed = self.reranker.rerank(
#                 ranked_df=ranked_df, post_metadata=post_meta, limit=limit
#             )
#             self.metrics["reranking_latency"].append((time.time() - t3) * 1000)

#             # Mark seen
#             try:
#                 self._mark_seen(user_id, [item["post_id"] for item in final_feed])
#             except Exception:
#                 pass

#             self.metrics["total_latency"].append((time.time() - start_time) * 1000)
#             logger.info(
#                 "Feed generated for user %s: %s posts | Latency: %.1fms",
#                 user_id,
#                 len(final_feed),
#                 self.metrics["total_latency"][-1],
#             )
#             return final_feed
#         except Exception as e:
#             logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
#             return []

#     def recommend_friends(self, user_id: int, k: int = 20) -> List[Dict]:
#         try:
#             from recommender.online.friend_recommendation import FriendRecommendation
#             fr = FriendRecommendation(
#                 data=self.data,
#                 embeddings=self.embeddings,
#                 cf_model=self.cf_model,
#                 config=self.config.get("friend_recommendation", {}) or {},
#             )
#             return fr.recommend_friends(user_id=user_id, k=k)
#         except Exception as e:
#             logger.error(f"Error recommending friends: {e}")
#             return []

#     def update_user_embedding_realtime(self, user_id: int, post_id: int, action: str):
#         conf = self.config.get("user_embedding", {}).get("real_time_update", {}) or {}
#         if not conf.get("enabled", False):
#             return
#         triggers = conf.get("trigger_actions", []) or []
#         if action not in triggers:
#             return
#         try:
#             if post_id not in (self.embeddings.get("post", {}) or {}):
#                 return
#             post_emb = self.embeddings["post"][post_id]
#             old_emb = (
#                 self.embeddings["user"][user_id]
#                 if user_id in (self.embeddings.get("user", {}) or {})
#                 else post_emb
#             )
#             alpha = conf.get("incremental", {}).get("learning_rate", 0.1)
#             weights = {"like": 1.0, "comment": 1.5, "share": 2.0, "save": 1.2, "view": 0.5}
#             a = alpha * weights.get(action, 1.0)
#             new_emb = (1 - a) * old_emb + a * post_emb
#             new_emb = new_emb / (np.linalg.norm(new_emb) + 1e-8)
#             self.embeddings.setdefault("user", {})[user_id] = new_emb.astype(np.float32)
#             if self.redis is not None:
#                 try:
#                     self.redis.setex(
#                         f"user:{user_id}:embedding", 7 * 24 * 3600, new_emb.tobytes()
#                     )
#                 except Exception:
#                     pass
#         except Exception as e:
#             logger.error(f"update_user_embedding_realtime error: {e}")

#     def get_metrics(self) -> Dict:
#         if not self.metrics["total_latency"]:
#             return {
#                 "total_requests": 0,
#                 "avg_latency_ms": 0,
#                 "p50_latency_ms": 0,
#                 "p95_latency_ms": 0,
#                 "p99_latency_ms": 0,
#                 "avg_recall_latency_ms": 0,
#                 "avg_ranking_latency_ms": 0,
#                 "avg_reranking_latency_ms": 0,
#             }
#         lat = self.metrics["total_latency"]
#         return {
#             "total_requests": len(lat),
#             "avg_latency_ms": float(np.mean(lat)),
#             "p50_latency_ms": float(np.percentile(lat, 50)),
#             "p95_latency_ms": float(np.percentile(lat, 95)),
#             "p99_latency_ms": float(np.percentile(lat, 99)),
#             "avg_recall_latency_ms": float(np.mean(self.metrics["recall_latency"])),
#             "avg_ranking_latency_ms": float(np.mean(self.metrics["ranking_latency"])),
#             "avg_reranking_latency_ms": float(np.mean(self.metrics["reranking_latency"])),
#         }

#     # --------------------------- INTERNALS ---------------------------
#     def _normalize_data_keys(self, raw: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
#         if raw is None:
#             return {}
#         data = dict(raw)
#         if "users" not in data and "user" in data: data["users"] = data["user"]
#         if "posts" not in data and "post" in data: data["posts"] = data["post"]
#         if "friendships" not in data and "friendship" in data: data["friendships"] = data["friendship"]
#         if "post_hashtags" not in data and "post_hashtag" in data: data["post_hashtags"] = data["post_hashtag"]
#         return data

#     def _recall_candidates(
#         self, user_id: int, target_count: int = 1000
#     ) -> Tuple[List[int], RecallDiag]:
#         all_candidates: List[int] = []
#         diag = RecallDiag(notes={})

#         channels_cfg = self.recall_config.get("channels", {}) or {}

#         # Following
#         try:
#             cnt = channels_cfg.get("following", {}).get("count", 400)
#             following_posts = self.following_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"FollowingRecall failed for user {user_id}: {e}")
#             following_posts = []
#         diag.following = len(following_posts)
#         all_candidates.extend(following_posts)

#         # CF
#         try:
#             cnt = channels_cfg.get("collaborative_filtering", {}).get("count", 300)
#             cf_posts = self.cf_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"CFRecall failed for user {user_id}: {e}")
#             cf_posts = []
#         diag.cf = len(cf_posts)
#         all_candidates.extend(cf_posts)

#         # Content
#         try:
#             cnt = channels_cfg.get("content_based", {}).get("count", 200)
#             content_posts = self.content_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"ContentRecall failed for user {user_id}: {e}")
#             content_posts = []
#         diag.content = len(content_posts)
#         all_candidates.extend(content_posts)

#         # Covisit
#         try:
#             cnt = channels_cfg.get("covisit", {}).get("count", 150)
#             covisit_posts = self.covisit_recall.recall(user_id, k=cnt)
#         except Exception as e:
#             logger.debug(f"CovisitRecall failed for user {user_id}: {e}")
#             covisit_posts = []
#         diag.covisit = len(covisit_posts)
#         all_candidates.extend(covisit_posts)

#         # Trending
#         try:
#             cnt = channels_cfg.get("trending", {}).get("count", 100)
#             trending_posts = self.trending_recall.recall(k=cnt)
#         except Exception as e:
#             logger.debug(f"TrendingRecall failed: {e}")
#             trending_posts = []
#         diag.trending = len(trending_posts)
#         all_candidates.extend(trending_posts)

#         unique_candidates = list(dict.fromkeys(all_candidates))[:target_count]
#         diag.total = len(unique_candidates)

#         logger.info(
#             "RECALL SUMMARY | user=%s | following=%s cf=%s content=%s covisit=%s trending=%s | total=%s",
#             user_id,
#             diag.following,
#             diag.cf,
#             diag.content,
#             diag.covisit,
#             diag.trending,
#             diag.total,
#         )

#         # Notes
#         flw_cnt = self._user_following_count(user_id)
#         if flw_cnt == 0 and diag.following == 0:
#             diag.notes["following"] = "User has no following"
#         tw = channels_cfg.get("trending", {}).get("trending_window_hours", 6)
#         if diag.trending == 0 and self._trending_empty_last_hours(tw):
#             diag.notes["trending"] = "No posts within trending window"
#         if diag.content == 0 and not self._has_user_embedding(user_id):
#             diag.notes["content"] = "User embedding missing (cold-start)"

#         return unique_candidates, diag

#     def _safe_text(self, v) -> str:
#         if v is None:
#             return ""
#         try:
#             if pd.isna(v):
#                 return ""
#         except Exception:
#             pass
#         if not isinstance(v, str):
#             return ""
#         return v.strip()

#     def _build_post_metadata(self, post_ids: List[int]) -> Dict[int, Dict]:
#         meta: Dict[int, Dict] = {}
#         try:
#             post_df: pd.DataFrame = self.data["posts"]
#             sub = post_df[post_df["Id"].isin(post_ids)][
#                 ["Id", "UserId", "Status", "CreateDate", "Content"]
#             ].copy()
#             for _, r in sub.iterrows():
#                 pid = int(r["Id"])
#                 meta[pid] = {
#                     "author_id": int(r.get("UserId")) if not pd.isna(r.get("UserId")) else None,
#                     "status": int(r.get("Status")) if not pd.isna(r.get("Status")) else None,
#                     "created_at": pd.to_datetime(r.get("CreateDate"), errors="coerce", utc=True),
#                     "title": "",
#                     "content": (r.get("Content") or ""),
#                     "content_hash": None,
#                 }
#         except Exception:
#             pass

#         missing = [pid for pid in post_ids if pid not in meta]
#         if missing and self.db_session_factory:
#             with self.db_session_factory() as s:
#                 rows = s.execute(
#                     text("SELECT Id, UserId, Status, CreateDate, Content FROM Post WHERE Id IN :ids"),
#                     {"ids": tuple(missing)},
#                 ).all()
#                 for row in rows:
#                     pid, uid, st, cd, ct = row
#                     meta[int(pid)] = {
#                         "author_id": int(uid) if uid is not None else None,
#                         "status": int(st) if st is not None else None,
#                         "created_at": pd.to_datetime(cd, errors="coerce", utc=True),
#                         "title": self._safe_text(None),
#                         "content": ct or "",
#                         "content_hash": None,
#                     }

#         import hashlib
#         for pid, m in meta.items():
#             if not m.get("content_hash"):
#                 title = self._safe_text(m.get("title")).lower()
#                 body = self._safe_text(m.get("content")).lower()
#                 if title or body:
#                     m["content_hash"] = hashlib.sha1(f"{title}|{body}".encode("utf-8")).hexdigest()
#         return meta

#     # ---------- seen-tracking (Redis ZSET) ----------
#     def _seen_key(self, user_id: int) -> str:
#         return f"user:{user_id}:seen_posts"

#     def _filter_seen(self, user_id: int, candidates: List[int]) -> List[int]:
#         if self.redis is None or not candidates:
#             return candidates
#         try:
#             key = self._seen_key(user_id)
#             seen_ids = set(int(x) for x in (self.redis.zrange(key, 0, -1) or []))
#             if not seen_ids:
#                 return candidates
#             return [p for p in candidates if p not in seen_ids]
#         except Exception:
#             return candidates

#     def _mark_seen(self, user_id: int, post_ids: List[int]) -> None:
#         if self.redis is None or not post_ids:
#             return
#         try:
#             key = self._seen_key(user_id)
#             ts = int(time.time())
#             with self.redis.pipeline() as p:
#                 for pid in post_ids:
#                     p.zadd(key, {int(pid): ts})
#                 p.expire(key, self.seen_ttl)
#                 p.execute()
#         except Exception:
#             pass

#     # ----- helpers for diag -----
#     def _user_following_count(self, user_id: int) -> int:
#         try:
#             if self.following_dict is None:
#                 return 0
#             flw = self.following_dict.get(user_id, [])
#             return len(flw) if flw is not None else 0
#         except Exception:
#             return 0

#     def _has_user_embedding(self, user_id: int) -> bool:
#         try:
#             return user_id in (self.embeddings.get("user", {}) or {})
#         except Exception:
#             return False

#     def _trending_empty_last_hours(self, hours: int) -> bool:
#         try:
#             post_df = self.data.get("posts")
#             if post_df is None or post_df.empty or "CreateDate" not in post_df.columns:
#                 return True
#             dt = pd.to_datetime(post_df["CreateDate"], errors="coerce", utc=True)
#             cutoff = pd.Timestamp.utcnow() - pd.Timedelta(hours=hours)
#             return bool((dt >= cutoff).sum() == 0)
#         except Exception:
#             return True

#     # ----- backend DB utilities -----
#     def ping_backend_db(self) -> bool:
#         if self.db_engine is None:
#             return False
#         try:
#             with self.db_engine.connect() as conn:
#                 conn.execute(text("SELECT 1"))
#             return True
#         except Exception:
#             return False

#     def backend_db_stats(self) -> dict:
#         if not self.db_session_factory:
#             return {"connected": False}
#         stats = {"connected": self.ping_backend_db()}
#         try:
#             with self.db_session_factory() as s:
#                 posts = s.execute(text("SELECT COUNT(1) FROM Post")).scalar_one()
#                 users = s.execute(text("SELECT COUNT(1) FROM User")).scalar_one()
#                 reactions = s.execute(text("SELECT COUNT(1) FROM PostReaction")).scalar_one()
#                 comments = s.execute(text("SELECT COUNT(1) FROM Comment")).scalar_one()
#                 last_post = s.execute(text("SELECT MAX(CreateDate) FROM Post")).scalar_one()
#                 stats.update(
#                     {
#                         "post_count": int(posts or 0),
#                         "user_count": int(users or 0),
#                         "reaction_count": int(reactions or 0),
#                         "comment_count": int(comments or 0),
#                         "last_post_at": str(last_post) if last_post else None,
#                     }
#                 )
#         except Exception as e:
#             stats.update({"error": str(e)})
#         return stats

#     def get_author_id(self, post_id: int) -> Optional[int]:
#         try:
#             row = self.data["posts"].loc[self.data["posts"]["Id"] == post_id]
#             if not row.empty:
#                 return int(row["UserId"].iloc[0])
#         except Exception:
#             pass
#         try:
#             if self.db_session_factory:
#                 with self.db_session_factory() as s:
#                     r = s.execute(text("SELECT UserId FROM Post WHERE Id=:pid"), {"pid": post_id}).first()
#                     return int(r[0]) if r else None
#         except Exception:
#             pass
#         return None



# recommender/online/online_inference_pipeline.py
# ======================================================================
# ONLINE INFERENCE PIPELINE ‚Äî PROD READY (seen-filter + quotas + fallback)
# Kh·ªõp offline artifacts (ranker_*.txt/pkl, latest.version) v√† load FAISS
# ======================================================================

from __future__ import annotations

import time
import logging
import warnings
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
import pickle
import numpy as np
import pandas as pd
import yaml

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)
logging.getLogger("lightgbm").setLevel(logging.WARNING)

# Project root
import sys
ROOT = Path(__file__).resolve().parents[2]
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))

# Artifacts + FE
from recommender.offline.artifact_manager import ArtifactManager, get_latest_version_dir
from recommender.common.feature_engineer import FeatureEngineer

# Recall
from recommender.online.recall import FollowingRecall, CFRecall, ContentRecall, TrendingRecall
from recommender.online.recall.covisit import CovisitRecall

# Ranking / Rerank
from recommender.online.ranking import MLRanker, Reranker

# Telemetry
from recommender.online.telemetry import AsyncDBLogger

# Optional Redis
try:
    import redis as _redis
    REDIS_AVAILABLE = True
except Exception:
    _redis = None
    REDIS_AVAILABLE = False
    logger.warning("redis not available")

# Optional DB
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# Optional FAISS
try:
    import faiss  # type: ignore
    FAISS_OK = True
except Exception:
    FAISS_OK = False


# -------------------------- Data classes ---------------------------
@dataclass
class RecallDiag:
    following: int = 0
    cf: int = 0
    content: int = 0
    covisit: int = 0
    trending: int = 0
    total: int = 0
    notes: Dict[str, str] = None


# ------------------------ Helpers (artifacts) -----------------------
def _load_ranker_artifacts(base_dir: str):
    """
    Load ranker artifacts theo format:
      models/<version>/
        - ranker_model.txt
        - ranker_scaler.pkl
        - ranker_feature_cols.pkl
        - meta.json (optional)
    T·ª± ƒë·ªông resolve latest.version ho·∫∑c symlink latest/
    """
    import json, pickle
    from lightgbm import Booster

    base = Path(base_dir)
    vdir = get_latest_version_dir(str(base))
    if vdir is None:
        latest_file = base / "latest.version"
        if latest_file.exists():
            ver = latest_file.read_text(encoding="utf-8").strip()
            vdir = base / ver
        elif (base / "latest").exists():
            try:
                vdir = (base / "latest").resolve(strict=True)
            except Exception:
                vdir = None
    if vdir is None:
        raise FileNotFoundError(f"Cannot resolve latest model version under {base_dir}")

    f_model = vdir / "ranker_model.txt"
    f_scaler = vdir / "ranker_scaler.pkl"
    f_cols  = vdir / "ranker_feature_cols.pkl"
    f_meta  = vdir / "meta.json"

    if not f_model.exists():
        raise FileNotFoundError(f"Missing model file: {f_model}")
    model = Booster(model_file=str(f_model))

    scaler = None
    if f_scaler.exists():
        with f_scaler.open("rb") as f:
            scaler = pickle.load(f)

    feature_cols = None
    if f_cols.exists():
        with f_cols.open("rb") as f:
            feature_cols = pickle.load(f)

    meta = {}
    if f_meta.exists():
        try:
            meta = json.loads(f_meta.read_text(encoding="utf-8"))
        except Exception:
            meta = {}

    return model, scaler, feature_cols, meta, vdir


# ========================== Main Pipeline ===========================
class OnlineInferencePipeline:
    """
    Flow: Recall (multi-channel, quotas, parallel) ‚Üí Seen-dedupe ‚Üí Ranking ‚Üí Re-ranking ‚Üí Mark Seen ‚Üí Output
    + Fallback n·∫øu recall r·ªóng (trending ‚Üí popular ‚Üí random recent)
    """

    def __init__(
        self,
        config_path: str = "configs/config_online.yaml",
        models_dir: str = "models",
        data_dir: str = "dataset",
        use_redis: bool = True,
    ):
        self.models_dir = Path(models_dir)
        
        self.data_dir = Path(data_dir)

        logger.info("\n" + "=" * 70)
        logger.info("INITIALIZING ONLINE INFERENCE PIPELINE")
        logger.info("=" * 70)
        logger.info(f"Using config: {config_path}")

        # ----------------- CONFIG -----------------
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f) or {}

        self.recall_config   = (self.config.get("recall") or {})
        self.ranking_config  = (self.config.get("ranking") or {})
        self.reranking_config= (self.config.get("reranking") or {})
        self.performance_cfg = (self.config.get("performance") or {})
        self.fallback_cfg    = (self.config.get("fallback") or {})
        self.redis_cfg       = (self.config.get("redis") or {})
        self.telemetry_cfg   = (self.config.get("telemetry") or {})

        # Sanity logs
        tr_cfg = (self.recall_config.get("channels") or {}).get("trending", {}) or {}
        logger.info(
            "Config sanity: trending_window_hours=%s | following.recent_hours=%s",
            tr_cfg.get("trending_window_hours"),
            (self.recall_config.get("channels") or {}).get("following",{}).get("recent_hours")
        )

        # TTLs
        ttl_cfg = (self.redis_cfg.get("ttl") or {})
        self.ttl_seen = int(ttl_cfg.get("seen_posts", 7 * 24 * 3600))  # default 7d

        # ----------------- REDIS ------------------
        self.redis: Optional[_redis.Redis] = None
        if use_redis and REDIS_AVAILABLE:
            try:
                url = self.redis_cfg.get("url")
                if url:
                    self.redis = _redis.from_url(url, decode_responses=True)
                else:
                    self.redis = _redis.Redis(
                        host=self.redis_cfg.get("host", "localhost"),
                        port=self.redis_cfg.get("port", 6379),
                        db=self.redis_cfg.get("db", 0),
                        decode_responses=True,
                        socket_timeout=self.redis_cfg.get("socket_timeout", 5),
                        max_connections=self.redis_cfg.get("max_connections", 50),
                    )
                self.redis.ping()
                logger.info("‚úÖ Redis connected")
            except Exception as e:
                logger.warning(f"Redis connection failed: {e}")
                self.redis = None
        else:
            logger.warning("Redis disabled or not available")

        # ----------------- BACKEND DB (optional) --------------
        self.db_engine = None
        self.db_session_factory = None
        try:
            db = (self.config.get("database") or self.config.get("backend_db") or {}) or {}
            db_url = db.get("url")
            if db_url:
                self.db_engine = create_engine(
                    db_url,
                    pool_size=db.get("pool_size", 20),
                    max_overflow=db.get("max_overflow", 40),
                    pool_recycle=db.get("pool_recycle", 1800),
                    pool_pre_ping=db.get("pool_pre_ping", True),
                    future=True,
                )
                self.db_session_factory = sessionmaker(bind=self.db_engine, autocommit=False, autoflush=False, future=True)
                with self.db_engine.connect() as conn:
                    conn.execute(text("SELECT 1"))
                logger.info("‚úÖ Backend DB connected")
        except Exception as e:
            logger.warning(f"Backend DB connection failed: {e}")

        # ----------------- ARTIFACTS (ranker) -----------------
        logger.info("\nüì¶ Loading model artifacts...")
        self.artifact_mgr = ArtifactManager(artifacts_base_dir=str(self.models_dir))
        req_version = (self.config.get("models") or {}).get("version", "latest")
        if req_version == "latest":
            vdir = self.artifact_mgr.get_latest_version_dir()
            if not vdir:
                raise RuntimeError("No model version found in models/ (missing latest.version)")
            self.current_version = vdir.name
        else:
            self.current_version = req_version

        self.ranking_model, self.ranking_scaler, self.ranking_feature_cols, self.metadata, self._version_dir = \
            _load_ranker_artifacts(base_dir=str(self.models_dir))
        logger.info("‚úÖ Ranker artifacts loaded")

        # ----------------- DATA (CSV/dev) ---------------------
        logger.info("\nüìä Loading data...")
        from recommender.common.data_loader import load_data
        raw = load_data(str(self.data_dir)) or {}
        self.data = self._normalize_data_keys(raw)
        logger.info("‚úÖ Data loaded")

        # ----------------- Optional embeddings / FAISS --------
        self.embeddings: Dict[str, Dict[int, np.ndarray]] = {"post": {}, "user": {}}
        self.faiss_index = None
        self.faiss_post_ids: List[int] = []
        self._load_optional_embeddings_file()   # post_embeddings.pkl n·∫øu c√≥
        self._load_faiss_artifacts()            # faiss_index.bin + mapping

        # ----------------- CF model placeholder ---------------
        self.cf_model = None  # n·∫øu b·∫°n c√≥ CF offline th√¨ n·∫°p ·ªü ƒë√¢y

        # ----------------- Stats / Following ------------------
        self.user_stats: Dict = {}
        self.author_stats: Dict = {}
        self.following_dict: Dict[int, set] = {}

        # ----------------- Feature Engineer -------------------
        self.feature_engineer = FeatureEngineer(
            data=self.data,
            user_stats=self.user_stats,
            author_stats=self.author_stats,
            following=self.following_dict,
            embeddings=self.embeddings,
        )

        # ----------------- Recalls ----------------------------
        channels_cfg = (self.recall_config.get("channels") or {})
        self.following_recall = FollowingRecall(
            redis_client=self.redis, data=self.data,
            following_dict=self.following_dict,
            config=channels_cfg.get("following", {}) or {}
        )
        self.cf_recall = CFRecall(
            redis_client=self.redis, data=self.data,
            cf_model=self.cf_model,
            config=channels_cfg.get("collaborative_filtering", {}) or {}
        )
        self.content_recall = ContentRecall(
            redis_client=self.redis, data=self.data,
            embeddings=self.embeddings,
            faiss_index=self.faiss_index, faiss_post_ids=self.faiss_post_ids,
            config=channels_cfg.get("content_based", {}) or {}
        )
        self.trending_recall = TrendingRecall(
            redis_client=self.redis, data=self.data,
            config=channels_cfg.get("trending", {}) or {}
        )
        self.covisit_recall = CovisitRecall(self.redis, k_per_anchor=25, max_anchors=5)

        # ----------------- Ranker / Reranker ------------------
        self.ranker = MLRanker(
            model=self.ranking_model,
            scaler=self.ranking_scaler,
            feature_cols=self.ranking_feature_cols,
            feature_engineer=self.feature_engineer,
            config=self.ranking_config,
        )
        rr = self.reranking_config
        if "freshness_enabled" in rr:
            rr.setdefault("freshness", {})["enabled"] = bool(rr.pop("freshness_enabled"))
        self.reranker = Reranker(config=rr)

        # ----------------- Telemetry (async) ------------------
        self.telemetry = AsyncDBLogger(self.telemetry_cfg, redis_client=self.redis)

        # ----------------- Final logs -------------------------
        if self.faiss_index is not None:
            logger.info("‚úÖ FAISS ready: %d vectors", getattr(self.faiss_index, "ntotal", 0))
        else:
            logger.warning("‚ö†Ô∏è  FAISS not loaded. Content recall will fallback to in-memory or return empty.\n"
                           "    Run: python -m recommender.offline.post_embeddings --mode full")

        logger.info("\n‚úÖ ONLINE PIPELINE READY!")
        logger.info("=" * 70 + "\n")

    # ====================== Public API ======================
    def generate_feed(self, user_id: int, limit: int = 50, exclude_seen: Optional[List[int]] = None, mark_seen: bool = False) -> List[Dict]:
        st = time.time()
        stage_ms = {"recall": 0, "ranking": 0, "rerank": 0}
        reasons: List[str] = []
        last_diag: Optional[RecallDiag] = None

        try:
            # ---------- Recall ----------
            t1 = time.time()
            target = int(self.recall_config.get("target_count", 1000))
            candidates, diag = self._recall_candidates(user_id, target_count=target)
            last_diag = diag
            stage_ms["recall"] = int((time.time() - t1) * 1000)

            # ---------- Seen-dedupe ----------
            seen = set(exclude_seen or [])
            seen |= self._get_seen_posts(user_id)
            if seen:
                before = len(candidates)
                candidates = [pid for pid in candidates if pid not in seen]
                if before and before != len(candidates):
                    reasons.append(f"seen_filtered:{before - len(candidates)}")

            # ---------- Fallback ----------
            if not candidates:
                fb = (self.fallback_cfg.get("personalization_failure") or {}).get("strategy", "trending")
                fallback = self._fallback_candidates(user_id, fb, k=max(limit * 3, 100))
                reasons.append(f"fallback:{fb}")
                candidates = fallback

            if not candidates:
                logger.warning("No candidates for user %s even after fallback", user_id)
                self._log_feed_telemetry(user_id, reasons, stage_ms, st, last_diag, final_count=0)
                return []

            # ---------- Ranking ----------
            t2 = time.time()
            ranked_df = self.ranker.rank(user_id, candidates)
            stage_ms["ranking"] = int((time.time() - t2) * 1000)
            if ranked_df.empty:
                reasons.append("ranking_empty")
                self._log_feed_telemetry(user_id, reasons, stage_ms, st, last_diag, final_count=0)
                return []

            top_k = int(self.ranking_config.get("top_k", 100))
            ranked_df = ranked_df.head(top_k)

            # ---------- Re-ranking ----------
            t3 = time.time()
            post_meta = self._build_post_metadata(ranked_df["post_id"].tolist())
            final_feed = self.reranker.rerank(ranked_df=ranked_df, post_metadata=post_meta, limit=limit)
            stage_ms["rerank"] = int((time.time() - t3) * 1000)

            # ---------- Mark Seen ----------
            if mark_seen: # Kh√¥ng auto mark-seen khi g·ªçi feed
                self._mark_seen_posts(user_id, [x["post_id"] for x in final_feed])

            # ---------- Telemetry ----------
            self._log_feed_telemetry(user_id, reasons, stage_ms, st, last_diag, final_count=len(final_feed))

            return final_feed

        except Exception as e:
            logger.error(f"Error generating feed for user {user_id}: {e}", exc_info=True)
            return []

    # ====================== Internals ========================
    def _normalize_data_keys(self, raw: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        if raw is None:
            return {}
        data = dict(raw)
        if "users" not in data and "user" in data: data["users"] = data["user"]
        if "posts" not in data and "post" in data: data["posts"] = data["post"]
        if "friendships" not in data and "friendship" in data: data["friendships"] = data["friendship"]
        if "post_hashtags" not in data and "post_hashtag" in data: data["post_hashtags"] = data["post_hashtag"]
        return data

    # ---------- FAISS & Embedding loaders ----------
    def _load_optional_embeddings_file(self):
        """
        N·∫°p models/post_embeddings.pkl (n·∫øu c√≥) ƒë·ªÉ d√πng cho m·ªôt s·ªë feature/diag.
        Kh√¥ng b·∫Øt bu·ªôc v√¨ ContentRecall s·∫Ω ∆∞u ti√™n d√πng FAISS.
        """
        print(self.models_dir)
        pkl = self.models_dir / "post_embeddings.pkl"
        if pkl.exists():
            try:
                with pkl.open("rb") as f:
                    obj = pickle.load(f)  # type: ignore
                # ch·∫•p nh·∫≠n 2 format: {post_id: vec} ho·∫∑c {'post':{...}, 'user':{...}}
                if isinstance(obj, dict) and "post" in obj:
                    self.embeddings.update(obj)
                else:
                    self.embeddings["post"] = obj  # type: ignore
                logger.info("Optional embeddings file loaded: %s", pkl)
            except Exception as e:
                logger.warning("Failed to load optional embeddings file: %s", e)

    def _load_faiss_artifacts(self):
        """
        N·∫°p FAISS index + mapping post_ids t·ª´ models/.
        """
        if not FAISS_OK:
            logger.warning("faiss not installed; content-based recall will be limited.")
            return
        print(self.models_dir)
        idx_path = self.models_dir / "faiss_index.bin"
        map_path = self.models_dir / "faiss_post_ids.pkl"
        if not (idx_path.exists() and map_path.exists()):
            logger.warning("FAISS artifacts not found in %s (missing faiss_index.bin or faiss_post_ids.pkl)", self.models_dir)
            return
        try:
            self.faiss_index = faiss.read_index(str(idx_path))  # type: ignore
            with map_path.open("rb") as f:
                self.faiss_post_ids = list(pickle.load(f))  # type: ignore
            # Sanity
            if hasattr(self.faiss_index, "ntotal"):
                if int(self.faiss_index.ntotal) != len(self.faiss_post_ids):
                    logger.warning("FAISS vectors (%s) != mapping size (%s)", int(self.faiss_index.ntotal), len(self.faiss_post_ids))
            logger.info("FAISS loaded: %d vectors", getattr(self.faiss_index, "ntotal", 0))
        except Exception as e:
            logger.warning("Failed to load FAISS artifacts: %s", e)
            self.faiss_index = None
            self.faiss_post_ids = []

    # ---------------- Recall orchestration -------------------

    # ---- NEW: recall following via Redis ZSET + DB backfill ----
    def _recall_following_redis(self, user_id: int, k: int) -> list[int]:
        out, seen = [], set()
        channels_cfg = (self.recall_config.get("channels") or {}).get("following", {}) or {}
        recent_hours = int(channels_cfg.get("recent_hours", 24 * 30))  # n·ªõi 30 ng√†y cho giai ƒëo·∫°n ƒë·∫ßu
        max_fetch    = int(channels_cfg.get("max_fetch", 5000))

        # 1) ∆Øu ti√™n t·ª´ ZSET following:{user_id}:posts (ƒë∆∞·ª£c webhook ƒë·∫©y)
        try:
            if self.redis:
                now = int(time.time())
                min_ts = now - recent_hours * 3600
                key = f"following:{user_id}:posts"
                rows = self.redis.zrevrangebyscore(key, max=now, min=min_ts, start=0, num=max_fetch) or []
                for s in rows:
                    pid = int(s)
                    if pid not in seen:
                        out.append(pid); seen.add(pid)
                    if len(out) >= k:
                        return out
        except Exception:
            pass

        # 2) Backfill t·ª´ danh s√°ch author m√† user follow (following_dict/DB)
        try:
            authors = list((self.following_dict or {}).get(user_id, []))
            if authors and "posts" in self.data:
                posts_df = self.data["posts"]
                sub = posts_df[posts_df["UserId"].isin(authors)].copy()
                if "CreateDate" in sub.columns:
                    dt = pd.to_datetime(sub["CreateDate"], errors="coerce", utc=True)
                    cutoff = pd.Timestamp.utcnow() - pd.Timedelta(hours=recent_hours)
                    sub = sub[dt >= cutoff]
                if "CreateDate" in sub.columns:
                    sub = sub.sort_values("CreateDate", ascending=False)
                for pid in sub["Id"].dropna().astype(int).tolist():
                    if pid not in seen:
                        out.append(pid); seen.add(pid)
                    if len(out) >= k:
                        break
        except Exception:
            pass

        return out[:k]



    def _recall_candidates(self, user_id: int, target_count: int = 1000) -> Tuple[List[int], RecallDiag]:
        from concurrent.futures import ThreadPoolExecutor, as_completed

        channels_cfg = self.recall_config.get("channels", {}) or {}
        timeout_ms = int(self.recall_config.get("timeout_ms", 500))
        parallel = bool((self.performance_cfg.get("parallel") or {}).get("recall_channels", True))

        # quotas
        q_follow  = int((channels_cfg.get("following", {}) or {}).get("count", 400))
        q_cf      = int((channels_cfg.get("collaborative_filtering", {}) or {}).get("count", 300))
        q_content = int((channels_cfg.get("content_based", {}) or {}).get("count", 200))
        q_trend   = int((channels_cfg.get("trending", {}) or {}).get("count", 100))
        q_covisit = int((channels_cfg.get("covisit", {}) or {}).get("count", 150))

        diag = RecallDiag(notes={})
        results: Dict[str, List[int]] = {"following": [], "cf": [], "content": [], "trending": [], "covisit": []}

        def _run(name: str, fn, k: int) -> Tuple[str, List[int]]:
            try:
                return name, fn(k=k)
            except Exception as e:
                logger.debug(f"{name} recall error: {e}")
                return name, []

        calls = [
            ("following", lambda kk: self._recall_following_redis(user_id, kk), q_follow),
            ("cf",        self.cf_recall.recall,        q_cf),
            ("content",   self.content_recall.recall,   q_content),
            ("covisit",   self.covisit_recall.recall,   q_covisit),
            ("trending",  self.trending_recall.recall,  q_trend),
        ]
        print(f"{calls} recall channels")
        if parallel:
            with ThreadPoolExecutor(max_workers=5) as ex:
                futs = {ex.submit(_run, n, f, k): n for (n, f, k) in calls}
                for ft in as_completed(futs, timeout=max(timeout_ms/1000.0, 0.05) + 5.0):
                    try:
                        n, posts = ft.result(timeout=max(timeout_ms/1000.0, 0.05))
                        results[n] = posts or []
                    except Exception:
                        pass
        else:
            for n, f, k in calls:
                name, posts = _run(n, f, k)
                results[name] = posts or []

        diag.following = len(results["following"])
        diag.cf        = len(results["cf"])
        diag.content   = len(results["content"])
        diag.covisit   = len(results["covisit"])
        diag.trending  = len(results["trending"])

        # unique preserve order
        all_candidates: List[int] = []
        for n in ["following", "cf", "content", "covisit", "trending"]:
            for pid in results[n]:
                if pid not in all_candidates:
                    all_candidates.append(pid)

        print(f"All candidates: {all_candidates}")
        unique_candidates = all_candidates[:target_count]
        diag.total = len(unique_candidates)

        logger.info(
            "RECALL SUMMARY | user=%s | following=%s cf=%s content=%s covisit=%s trending=%s | total=%s",
            user_id, diag.following, diag.cf, diag.content, diag.covisit, diag.trending, diag.total,
        )

        # Notes
        if diag.following == 0 and self._user_following_count(user_id) == 0:
            diag.notes["following"] = "User has no following"
        if diag.trending == 0:
            tw = (channels_cfg.get("trending", {}) or {}).get("trending_window_hours", 6)
            if self._trending_empty_last_hours(tw):
                diag.notes["trending"] = "No posts within trending window"
        if diag.content == 0 and not self._has_user_embedding(user_id):
            diag.notes["content"] = "User embedding missing (cold start)"
        if self.faiss_index is None:
            diag.notes["content"] = (diag.notes.get("content","") + " | FAISS missing").strip(" |")

        return unique_candidates, diag

    # -------------------------- Fallback --------------------------
    def _fallback_candidates(self, user_id: int, strategy: str, k: int) -> List[int]:
        strategy = (strategy or "trending").lower()
        if strategy == "trending":
            posts = self.trending_recall.recall(k=k)
            if posts:
                return posts
        if strategy == "popular":
            try:
                r = self.redis
                keys = r.scan_iter(match="post:*:eng1h", count=1000) if r else []
                scores = []
                for key in keys or []:
                    h = r.hgetall(key) if r else {}
                    sc = sum(float(v or 0) for v in h.values()) if h else 0.0
                    if sc > 0:
                        pid = int(key.split(":")[1])
                        scores.append((sc, pid))
                scores.sort(reverse=True)
                return [pid for _, pid in scores[:k]]
            except Exception:
                pass
        # random recent if nothing else
        try:
            posts_df = self.data.get("posts")
            if posts_df is not None and not posts_df.empty:
                sub = posts_df.copy()
                if "CreateDate" in sub.columns:
                    dt = pd.to_datetime(sub["CreateDate"], errors="coerce", utc=True)
                    cutoff = pd.Timestamp.utcnow() - pd.Timedelta(days=7)
                    sub = sub[dt >= cutoff]
                pids = sub["Id"].dropna().astype(int).tolist()
                np.random.shuffle(pids)
                return pids[:k]
        except Exception:
            pass
        return []

    # ---------------------- Seen tracking -----------------------
    def _seen_key(self, user_id: int) -> str:
        return f"user:{user_id}:seen_posts"

    def _get_seen_posts(self, user_id: int) -> set:
        if not self.redis:
            return set()
        try:
            rows = self.redis.zrevrange(self._seen_key(user_id), 0, 2000)
            return set(int(x) for x in rows) if rows else set()
        except Exception:
            return set()

    def _mark_seen_posts(self, user_id: int, post_ids: List[int]):
        if not (self.redis and post_ids):
            return
        try:
            now = int(time.time())
            key = self._seen_key(user_id)
            pipe = self.redis.pipeline()
            for pid in post_ids:
                pipe.zadd(key, {str(int(pid)): now})
            pipe.expire(key, self.ttl_seen)
            pipe.execute()
        except Exception:
            pass

    # ---------------------- Post metadata -----------------------
    def _safe_text(self, v) -> str:
        if v is None:
            return ""
        try:
            if pd.isna(v):
                return ""
        except Exception:
            pass
        return str(v).strip()

    def _build_post_metadata(self, post_ids: List[int]) -> Dict[int, Dict]:
        meta: Dict[int, Dict] = {}

        # 1) CSV/dev
        try:
            post_df: pd.DataFrame = self.data["posts"]
            sub = post_df[post_df["Id"].isin(post_ids)][["Id", "UserId", "Status", "CreateDate", "Content"]].copy()
            for _, r in sub.iterrows():
                pid = int(r["Id"])
                content = self._safe_text(r.get("Content"))
                title = ""
                content_hash = None
                if content or title:
                    import hashlib
                    content_hash = hashlib.sha1(f"{title.lower()}|{content.lower()}".encode("utf-8")).hexdigest()
                meta[pid] = {
                    "author_id": int(r.get("UserId")) if not pd.isna(r.get("UserId")) else None,
                    "status": int(r.get("Status")) if not pd.isna(r.get("Status")) else None,
                    "created_at": pd.to_datetime(r.get("CreateDate"), errors="coerce", utc=True),
                    "title": title,
                    "content": content,
                    "content_hash": content_hash,
                }
        except Exception:
            pass

        # 2) DB fallback
        missing = [pid for pid in post_ids if pid not in meta]
        if missing and self.db_session_factory:
            with self.db_session_factory() as s:
                rows = s.execute(
                    text("SELECT Id, UserId, Status, CreateDate, Content FROM Post WHERE Id IN :ids"),
                    {"ids": tuple(missing)},
                ).all()
                for row in rows:
                    pid, uid, st, cd, ct = row
                    content = self._safe_text(ct)
                    title = ""
                    content_hash = None
                    if content or title:
                        import hashlib
                        content_hash = hashlib.sha1(f"{title.lower()}|{content.lower()}".encode("utf-8")).hexdigest()
                    meta[int(pid)] = {
                        "author_id": int(uid) if uid is not None else None,
                        "status": int(st) if st is not None else None,
                        "created_at": pd.to_datetime(cd, errors="coerce", utc=True),
                        "title": title,
                        "content": content,
                        "content_hash": content_hash,
                    }
        return meta

    # -------------------- Diag utils ------------------
    def _user_following_count(self, user_id: int) -> int:
        try:
            flw = (self.following_dict or {}).get(user_id)
            return len(flw) if flw else 0
        except Exception:
            return 0

    def _has_user_embedding(self, user_id: int) -> bool:
        try:
            return user_id in (self.embeddings.get("user", {}) or {})
        except Exception:
            return False

    def _trending_empty_last_hours(self, hours: int) -> bool:
        try:
            post_df = self.data.get("posts")
            if post_df is None or post_df.empty or "CreateDate" not in post_df.columns:
                return True
            dt = pd.to_datetime(post_df["CreateDate"], errors="coerce", utc=True)
            cutoff = pd.Timestamp.utcnow() - pd.Timedelta(hours=hours)
            return bool((dt >= cutoff).sum() == 0)
        except Exception:
            return True

    # ------------------------ Telemetry ------------------------
    def _log_feed_telemetry(self, user_id: int, reasons: List[str], stage_ms: Dict[str, int],
                            st: float, diag: Optional[RecallDiag], final_count: int):
        try:
            if not self.telemetry or not getattr(self.telemetry, "enabled", False):
                return
            data = {
                "user_id": user_id,
                "timestamp": time.time(),
                "recall": {
                    "following": (diag.following if diag else 0),
                    "cf": (diag.cf if diag else 0),
                    "content": (diag.content if diag else 0),
                    "trending": (diag.trending if diag else 0),
                    "total": (diag.total if diag else 0),
                },
                "final_count": final_count,
                "reasons": reasons,
                "latency_ms": int((time.time() - st) * 1000),
                "stage_ms": stage_ms,
            }
            self.telemetry.log_feed_request(data)
        except Exception:
            pass
